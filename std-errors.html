<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Econometrics with R</title>
  <meta name="description" content="SciencesPo UG Econometrics online textbook. Almost no Maths.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Econometrics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://scpoecon.github.io/ScPoEconometrics/" />
  
  <meta property="og:description" content="SciencesPo UG Econometrics online textbook. Almost no Maths." />
  <meta name="github-repo" content="ScPoEcon/ScPoEconometrics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Econometrics with R" />
  
  <meta name="twitter:description" content="SciencesPo UG Econometrics online textbook. Almost no Maths." />
  

<meta name="author" content="Florian Oswald, Jean-Marc Robin and Vincent Viers">


<meta name="date" content="2018-12-03">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="favicon.gif" type="image/x-icon">
<link rel="prev" href="categorical-vars.html">
<link rel="next" href="quantreg.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-41584331-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-41584331-4');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ScPo 2nd Year Econometrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="1" data-path="R-intro.html"><a href="R-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to <code>R</code></a><ul>
<li class="chapter" data-level="1.1" data-path="R-intro.html"><a href="R-intro.html#getting-started"><i class="fa fa-check"></i><b>1.1</b> Getting Started</a></li>
<li class="chapter" data-level="1.2" data-path="R-intro.html"><a href="R-intro.html#starting-r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> Starting R and RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="R-intro.html"><a href="R-intro.html#basic-calculations"><i class="fa fa-check"></i><b>1.3</b> Basic Calculations</a></li>
<li class="chapter" data-level="1.4" data-path="R-intro.html"><a href="R-intro.html#getting-help"><i class="fa fa-check"></i><b>1.4</b> Getting Help</a></li>
<li class="chapter" data-level="1.5" data-path="R-intro.html"><a href="R-intro.html#installing-packages"><i class="fa fa-check"></i><b>1.5</b> Installing Packages</a></li>
<li class="chapter" data-level="1.6" data-path="R-intro.html"><a href="R-intro.html#code-output"><i class="fa fa-check"></i><b>1.6</b> <code>Code</code> vs Output in this Book</a></li>
<li class="chapter" data-level="1.7" data-path="R-intro.html"><a href="R-intro.html#install-package"><i class="fa fa-check"></i><b>1.7</b> <code>ScPoEconometrics</code> Package</a></li>
<li class="chapter" data-level="1.8" data-path="R-intro.html"><a href="R-intro.html#data-types"><i class="fa fa-check"></i><b>1.8</b> Data Types</a></li>
<li class="chapter" data-level="1.9" data-path="R-intro.html"><a href="R-intro.html#data-structures"><i class="fa fa-check"></i><b>1.9</b> Data Structures</a></li>
<li class="chapter" data-level="1.10" data-path="R-intro.html"><a href="R-intro.html#dataframes"><i class="fa fa-check"></i><b>1.10</b> Data Frames</a></li>
<li class="chapter" data-level="1.11" data-path="R-intro.html"><a href="R-intro.html#programming-basics"><i class="fa fa-check"></i><b>1.11</b> Programming Basics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sum.html"><a href="sum.html"><i class="fa fa-check"></i><b>2</b> Working With Data</a><ul>
<li class="chapter" data-level="2.1" data-path="sum.html"><a href="sum.html#summary-statistics"><i class="fa fa-check"></i><b>2.1</b> Summary Statistics</a></li>
<li class="chapter" data-level="2.2" data-path="sum.html"><a href="sum.html#plotting"><i class="fa fa-check"></i><b>2.2</b> Plotting</a></li>
<li class="chapter" data-level="2.3" data-path="sum.html"><a href="sum.html#summarize-two"><i class="fa fa-check"></i><b>2.3</b> Summarizing Two Variables</a></li>
<li class="chapter" data-level="2.4" data-path="sum.html"><a href="sum.html#the-tidyverse"><i class="fa fa-check"></i><b>2.4</b> The <code>tidyverse</code></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linreg.html"><a href="linreg.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linreg.html"><a href="linreg.html#how-are-x-and-y-related"><i class="fa fa-check"></i><b>3.1</b> How are <code>x</code> and <code>y</code> related?</a></li>
<li class="chapter" data-level="3.2" data-path="linreg.html"><a href="linreg.html#OLS"><i class="fa fa-check"></i><b>3.2</b> Ordinary Least Squares (OLS) Estimator</a></li>
<li class="chapter" data-level="3.3" data-path="linreg.html"><a href="linreg.html#pred-resids"><i class="fa fa-check"></i><b>3.3</b> Predictions and Residuals</a></li>
<li class="chapter" data-level="3.4" data-path="linreg.html"><a href="linreg.html#correlation-covariance-and-linearity"><i class="fa fa-check"></i><b>3.4</b> Correlation, Covariance and Linearity</a></li>
<li class="chapter" data-level="3.5" data-path="linreg.html"><a href="linreg.html#analysing-vary"><i class="fa fa-check"></i><b>3.5</b> Analysing <span class="math inline">\(Var(y)\)</span></a></li>
<li class="chapter" data-level="3.6" data-path="linreg.html"><a href="linreg.html#assessing-the-goodness-of-fit"><i class="fa fa-check"></i><b>3.6</b> Assessing the <em>Goodness of Fit</em></a></li>
<li class="chapter" data-level="3.7" data-path="linreg.html"><a href="linreg.html#lm-example1"><i class="fa fa-check"></i><b>3.7</b> An Example: California Student Test Scores</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-reg.html"><a href="multiple-reg.html"><i class="fa fa-check"></i><b>4</b> Multiple Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="multiple-reg.html"><a href="multiple-reg.html#ceteris"><i class="fa fa-check"></i><b>4.1</b> All Else Equal</a></li>
<li class="chapter" data-level="4.2" data-path="multiple-reg.html"><a href="multiple-reg.html#multicol"><i class="fa fa-check"></i><b>4.2</b> Multicolinearity</a></li>
<li class="chapter" data-level="4.3" data-path="multiple-reg.html"><a href="multiple-reg.html#california-test-scores-2"><i class="fa fa-check"></i><b>4.3</b> California Test Scores 2</a></li>
<li class="chapter" data-level="4.4" data-path="multiple-reg.html"><a href="multiple-reg.html#mreg-interactions"><i class="fa fa-check"></i><b>4.4</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="categorical-vars.html"><a href="categorical-vars.html"><i class="fa fa-check"></i><b>5</b> Categorial Variables</a><ul>
<li class="chapter" data-level="5.1" data-path="categorical-vars.html"><a href="categorical-vars.html#the-binary-regressor-case"><i class="fa fa-check"></i><b>5.1</b> The Binary Regressor Case</a></li>
<li class="chapter" data-level="5.2" data-path="categorical-vars.html"><a href="categorical-vars.html#dummy-and-continuous-variables"><i class="fa fa-check"></i><b>5.2</b> Dummy and Continuous Variables</a></li>
<li class="chapter" data-level="5.3" data-path="categorical-vars.html"><a href="categorical-vars.html#categorical-variables-in-r-factor"><i class="fa fa-check"></i><b>5.3</b> Categorical Variables in <code>R</code>: <code>factor</code></a></li>
<li class="chapter" data-level="5.4" data-path="categorical-vars.html"><a href="categorical-vars.html#saturated-models-main-effects-and-interactions"><i class="fa fa-check"></i><b>5.4</b> Saturated Models: Main Effects and Interactions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="std-errors.html"><a href="std-errors.html"><i class="fa fa-check"></i><b>6</b> Standard Errors</a><ul>
<li class="chapter" data-level="6.1" data-path="std-errors.html"><a href="std-errors.html#what-is-true-what-are-statistical-models"><i class="fa fa-check"></i><b>6.1</b> What is <em>true</em>? What are Statistical Models?</a></li>
<li class="chapter" data-level="6.2" data-path="std-errors.html"><a href="std-errors.html#class-reg"><i class="fa fa-check"></i><b>6.2</b> The Classical Regression Model</a></li>
<li class="chapter" data-level="6.3" data-path="std-errors.html"><a href="std-errors.html#back-to-sampling"><i class="fa fa-check"></i><b>6.3</b> Back to Sampling</a></li>
<li class="chapter" data-level="6.4" data-path="std-errors.html"><a href="std-errors.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.4</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.5" data-path="std-errors.html"><a href="std-errors.html#whats-in-my-model-and-what-is-not"><i class="fa fa-check"></i><b>6.5</b> What’s in my model? (And what is not?)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="quantreg.html"><a href="quantreg.html"><i class="fa fa-check"></i><b>7</b> Quantile Regression</a></li>
<li class="chapter" data-level="8" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>8</b> Panel Data</a><ul>
<li class="chapter" data-level="8.1" data-path="panel-data.html"><a href="panel-data.html#fixed-effects"><i class="fa fa-check"></i><b>8.1</b> fixed effects</a></li>
<li class="chapter" data-level="8.2" data-path="panel-data.html"><a href="panel-data.html#did"><i class="fa fa-check"></i><b>8.2</b> DiD</a></li>
<li class="chapter" data-level="8.3" data-path="panel-data.html"><a href="panel-data.html#rdd"><i class="fa fa-check"></i><b>8.3</b> RDD</a></li>
<li class="chapter" data-level="8.4" data-path="panel-data.html"><a href="panel-data.html#example"><i class="fa fa-check"></i><b>8.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="IV.html"><a href="IV.html"><i class="fa fa-check"></i><b>9</b> Instrumental Variables</a><ul>
<li class="chapter" data-level="9.1" data-path="IV.html"><a href="IV.html#simultaneity-bias"><i class="fa fa-check"></i><b>9.1</b> Simultaneity Bias</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logit-probit.html"><a href="logit-probit.html"><i class="fa fa-check"></i><b>10</b> Logit and Probit</a></li>
<li class="chapter" data-level="11" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>11</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="12" data-path="R-advanced.html"><a href="R-advanced.html"><i class="fa fa-check"></i><b>12</b> Advanced <code>R</code></a><ul>
<li class="chapter" data-level="12.1" data-path="R-advanced.html"><a href="R-advanced.html#more-vectorization"><i class="fa fa-check"></i><b>12.1</b> More Vectorization</a></li>
<li class="chapter" data-level="12.2" data-path="R-advanced.html"><a href="R-advanced.html#calculations-with-vectors-and-matrices"><i class="fa fa-check"></i><b>12.2</b> Calculations with Vectors and Matrices</a></li>
<li class="chapter" data-level="12.3" data-path="R-advanced.html"><a href="R-advanced.html#matrices-1"><i class="fa fa-check"></i><b>12.3</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="slides.html"><a href="slides.html"><i class="fa fa-check"></i><b>13</b> Slides</a><ul>
<li class="chapter" data-level="13.1" data-path="slides.html"><a href="slides.html#worked-tutorials"><i class="fa fa-check"></i><b>13.1</b> Worked Tutorials</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>14</b> Notes</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="std-errors" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Standard Errors</h1>
<p>In the previous chapters we have seen how the OLS method can produce estimates about intercept and slope coefficients from data. You have seen this method at work in <code>R</code> by using the <code>lm</code> function as well. It is now time to introduce the notion that given that <span class="math inline">\(b_0\)</span>, <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are <em>estimates</em> of some unkown <em>population parameters</em>, there is some degree of <strong>uncertainty</strong> about their values. An other way to say this is that we want some indication about the <em>precision</em> of those estimates.</p>
<div class="note">
<center>
How <em>confident</em> should we be about the estimated values <span class="math inline"><span class="math inline">\(b\)</span></span>?
</center>
</div>
<p><br> Let’s go back to the regression with one variable and remind ourselves of the example at the end of chapter <a href="linreg.html#linreg">3</a>. There we introduced the term <em>confidence interval</em>, shown as the shaded area in figure <a href="std-errors.html#fig:confint">6.1</a>:</p>
<div class="figure" style="text-align: center"><span id="fig:confint"></span>
<img src="ScPoEconometrics_files/figure-html/confint-1.png" alt="Confidence bands around a regression line." width="672" />
<p class="caption">
Figure 6.1: Confidence bands around a regression line.
</p>
</div>
<p>The shaded area shows us the region within which the <strong>true</strong> red line will lie with 95% probability. The fact that there is an unknown true line (i.e. a <em>true</em> slope coefficient <span class="math inline">\(\beta_1\)</span>) that we wish to uncover from a sample of data should remind you immediately of our first tutorial. There, we wanted to estimate the true population mean from a sample of data, and we saw that as the sample size <span class="math inline">\(N\)</span> increased, our estimate got better and better - fundamentally this is the same idea here.</p>
<div id="what-is-true-what-are-statistical-models" class="section level2">
<h2><span class="header-section-number">6.1</span> What is <em>true</em>? What are Statistical Models?</h2>
<p>A <strong>statistical model</strong> is simply a set of assumptions about how some data have been generated. As such, it models the data-generating process (DGP), as we have it in mind. Once we define a DGP, we could simulate data from it and see how this compares to the data we observe in the real world. Or, we could change the parameters of the DGP so as to understand how the real world data <em>would</em> change, could we (or some policy) change the corresponding parameters in reality. Let us now consider one particular statistical model, which in fact we have seen so many times already.</p>
</div>
<div id="class-reg" class="section level2">
<h2><span class="header-section-number">6.2</span> The Classical Regression Model</h2>
<p>Let’s bring back our simple model <a href="linreg.html#eq:abline">(3.3)</a> to explain this concept.</p>
<span class="math display" id="eq:abline-5">\[\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \tag{6.1}
\end{equation}\]</span>
<p>The smallest set of assumptions used to define the <em>classical regression model</em> as in <a href="std-errors.html#eq:abline-5">(6.1)</a> are the following:</p>
<ol style="list-style-type: decimal">
<li>The data are <strong>not linearly dependent</strong>: Each variable provides new information for the outcome, and it cannot be replicated as a linear combination of other variables. We have seen this in section <a href="multiple-reg.html#multicol">4.2</a>. In the particular case of one regressor, as here, we require that <span class="math inline">\(x\)</span> exhibit some variation in the data, i.e. <span class="math inline">\(Var(x)\neq 0\)</span>.</li>
<li>The mean of the residuals conditional on <span class="math inline">\(x\)</span> should be zero, <span class="math inline">\(E[\varepsilon|x] = 0\)</span>. Notice that this also means that <span class="math inline">\(Cov(\varepsilon,x) = 0\)</span>, i.e. that the errors and our explanatory variable(s) should be <em>uncorrelated</em>. It is said that <span class="math inline">\(x\)</span> should be <strong>strictly exogenous</strong> to the model.</li>
</ol>
<p>These assumptions are necessary to successfully (and correctly!) run an OLS regression. They are often supplemented with an additional set of assumptions, which help with certain aspects of the exposition, but are not strictly necessary:</p>
<ol start="3" style="list-style-type: decimal">
<li>The data are drawn from a <strong>random sample</strong> of size <span class="math inline">\(n\)</span>: observation <span class="math inline">\((x_i,y_i)\)</span> comes from the exact same distribution, and is independent of observation <span class="math inline">\((x_j,y_j)\)</span>, for all <span class="math inline">\(i\neq j\)</span>.</li>
<li>The variance of the error term <span class="math inline">\(\varepsilon\)</span> is the same for each value of <span class="math inline">\(x\)</span>: <span class="math inline">\(Var(\varepsilon|x) = \sigma^2\)</span>. This property is called <strong>homoskedasticity</strong>.</li>
<li>The error is normally distributed, i.e. <span class="math inline">\(\varepsilon \sim \mathcal{N}(0,\sigma^2)\)</span></li>
</ol>
<p>Invoking assumption 5. in particular defines what is commonly called the <em>normal</em> linear regression model.</p>
<div id="b-is-not-beta" class="section level3">
<h3><span class="header-section-number">6.2.1</span> <span class="math inline">\(b\)</span> is not <span class="math inline">\(\beta\)</span>!</h3>
<p>Let’s talk about the small but important modifications we applied to model <a href="linreg.html#eq:abline">(3.3)</a> to end up at <a href="std-errors.html#eq:abline-5">(6.1)</a> above:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and intercept and slope parameters</li>
<li><span class="math inline">\(\varepsilon\)</span> is the error term.</li>
</ul>
<p>First, we <em>assumed</em> that <a href="std-errors.html#eq:abline-5">(6.1)</a> is the correct represenation of the DGP. With that assumption in place, the values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the <em>true parameter values</em> which generated the data. Notice that <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are potentially different from <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> in <a href="linreg.html#eq:abline">(3.3)</a> for a given sample of data - they could in practice be very close to each other, but <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are <em>estimates</em> of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. And, crucially, those estimates are generated from a sample of data. Now, the fact that our data <span class="math inline">\(\{y_i,x_i\}_{i=1}^N\)</span> are a sample from a larger population, means that there will be <em>sampling variation</em> in our estimates - exactly like in the case of the sample mean estimating the population average as mentioned above. One particular sample of data will generate one particular set of estimates <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, whereas another sample of data will generate estimates which will in general be different - by <em>how much</em> those estimates differ across samples is the question in this chapter. In general, the more observations we have the greater the precision of our estimates, hence, the closer the estimates from different samples will lie together.</p>
</div>
<div id="se-theory" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Standard Errors in Theory</h3>
<p>The standard deviation of the OLS parameters is generally called <em>standard error</em>. As such, it is just the square root of the parameter’s variance. Under assumptions 1. through 4. we can define the formula for the variance of our slope coefficient in the context of our single regressor model <a href="std-errors.html#eq:abline-5">(6.1)</a> as follows:</p>
<span class="math display" id="eq:var-ols">\[\begin{equation}
Var(b_1|x_i) = \frac{\sigma^2}{\sum_i^N (x_i - \bar{x})^2}  \tag{6.2}
\end{equation}\]</span>
<p>In pratice, we don’t know the theoretical variance of <span class="math inline">\(\varepsilon\)</span>, i.e. <span class="math inline">\(\sigma^2\)</span>, but we form an estimate about it from our sample of data. A widely used estimate uses the already encountered SSR (sum of squared residuals), and is denoted <span class="math inline">\(s^2\)</span>:</p>
<p><span class="math display">\[
s^2 = \frac{SSR}{n-p} = \frac{\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2}{n-p} =  \frac{\sum_{i=1}^n e_i^2}{n-p}
\]</span> where <span class="math inline">\(n-p\)</span> are the <em>degrees of freedom</em> available in this estimation. <span class="math inline">\(p\)</span> is the number of parameters we wish to estimate (here: 1). So, the variance formula would become</p>
<span class="math display" id="eq:var-ols2">\[\begin{equation}
Var(b_1|x_i) = \frac{SSR}{(n-p)\sum_i^N (x_i - \bar{x})^2}  \tag{6.3}
\end{equation}\]</span>
<p>We most of the time work directly with the <em>standard error</em> of a coefficient, hence we define</p>
<span class="math display" id="eq:SE-ols2">\[\begin{equation}
SE(b_1) = \sqrt{Var(b_1|x_i)} = \sqrt{\frac{SSR}{(n-p)\sum_i^N (x_i - \bar{x})^2}}  \tag{6.4}
\end{equation}\]</span>
<p>You can clearly see that, as <span class="math inline">\(n\)</span> increases, the denominator increases, and therefore variance and standard error of the estimate will decrease.</p>
</div>
<div id="standard-errors-in-practice" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Standard Errors in Practice</h3>
<p>We would like to further make this point in an experiential way, i.e. we want you to experience what is going on. We invite you to spend some time with the following apps. In particular, make sure you have a thorough understanding of <code>launchApp(&quot;estimate&quot;)</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ScPoEconometrics)
<span class="kw">launchApp</span>(<span class="st">&quot;estimate&quot;</span>)
<span class="kw">launchApp</span>(<span class="st">&quot;sampling&quot;</span>)  
<span class="kw">launchApp</span>(<span class="st">&quot;standard_errors_simple&quot;</span>) 
<span class="kw">launchApp</span>(<span class="st">&quot;standard_errors_changeN&quot;</span>) </code></pre></div>
</div>
</div>
<div id="back-to-sampling" class="section level2">
<h2><span class="header-section-number">6.3</span> Back to Sampling</h2>
<p>Imagine we were tasked by the Director of our school to provide him with our best guess of the <em>mean body height</em> <span class="math inline">\(\mu\)</span> amongst all SciencesPo students in order to assess which height the new desks should have. Of course, we are econometricians and don’t <em>guess</em> things: we <strong>estimate</strong> them! How would we go about this task and estimate <span class="math inline">\(\mu\)</span>?</p>
<p>You may want to ask: Why bother with this estimation business at all, and not just measure all students’ height, compute <span class="math inline">\(\mu\)</span>, and that’s it? That’s a good question! In most cases, we cannot do this, either because we do not have access to the entire population (think of computing the mean height of all Europeans!), or it’s too costly to measure everyone, or it’s impractical. That’s why we take <em>samples</em> from the wider population, to make inference. In our example, suppose we’d randomly measure students coming out of the SciencesPo building at 27 Rue Saint Guillaume until we have <span class="math inline">\(50\)</span> measurements on any given Monday. Suppose further that we found a sample mean height <span class="math inline">\(\bar{x} = 168.5\)</span>, and that the sample standard deviation was <span class="math inline">\(s=10\)</span>. In short, we found the data summarized in figure <a href="std-errors.html#fig:heightdata">6.2</a></p>
<div class="figure" style="text-align: center"><span id="fig:heightdata"></span>
<img src="ScPoEconometrics_files/figure-html/heightdata-1.png" alt="Our ficitious sample of SciencesPo students' body height. The small ticks indicate the location of each measurement." width="672" />
<p class="caption">
Figure 6.2: Our ficitious sample of SciencesPo students’ body height. The small ticks indicate the location of each measurement.
</p>
</div>
<p>What are we going to tell <em>Monsieur le Directeur</em> now, with those two numbers and figure <a href="std-errors.html#fig:heightdata">6.2</a> in hand? Before we address this issue, we need to make a short detour into <em>test statistics</em>.</p>
<div id="test-statistics" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Test Statistics</h3>
<p>We have encountered many statistics already: think of the sample mean, or the standard deviation. Statistics are just functions of data. <em>Test</em> statistics are used to perform statistical tests.</p>
<p>Many test statistics rely on some notion of <em>standardizing</em> the sample data so that it becomes comparable to a theoretical distribution. We encountered this idea already in section <a href="linreg.html#reg-standard">3.2.4</a>, where we talked about a standardized regression. The most common standardization is the so-called <em>z-score</em>, which says that</p>
<span class="math display" id="eq:zscore">\[\begin{equation}
\frac{x - \mu}{\sigma}\equiv z\sim \mathcal{N}(0,1), \tag{6.5}
\end{equation}\]</span>
<p>in other words, substracting the population mean from random variable <span class="math inline">\(x\)</span> and dividing by it’s population standard deviation yields a standard normally distributed random variable, commonly called <span class="math inline">\(z\)</span>.</p>
<p>A very similar idea applies if we <em>don’t know</em> the population variance (which is our case here!). The corresponding standardization gives rise to the <em>t-statistic</em>, and it looks very similar to <a href="std-errors.html#eq:zscore">(6.5)</a>:</p>
<span class="math display" id="eq:tscore">\[\begin{equation}
\sqrt{n} \frac{\bar{x} - \mu}{s} \equiv T \sim t_{n-1} \tag{6.6}
\end{equation}\]</span>
<p>Several things to note:</p>
<ul>
<li>We observe the same standardization as above: dividing by the sample standard deviation <span class="math inline">\(s\)</span> brings <span class="math inline">\(\bar{x} - \mu\)</span> to a <em>unit free</em> scale.</li>
<li>We use <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(s\)</span> instead of <span class="math inline">\(x\)</span> and <span class="math inline">\(\sigma\)</span></li>
<li>We multiply by <span class="math inline">\(\sqrt{n}\)</span> because we expect <span class="math inline">\(\bar{x} - \mu\)</span> to be a small number: we need to <em>rescale</em> it again to make it compatible with the <span class="math inline">\(t_{n-1}\)</span> distribution.</li>
<li><span class="math inline">\(t_{n-1}\)</span> is the <a href="https://en.wikipedia.org/wiki/Student&#39;s_t-distribution">Student’s T</a> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. We don’t have <span class="math inline">\(n\)</span> degrees of freedom because we already had to estimate one statistic (<span class="math inline">\(\bar{x}\)</span>) in order to construct <span class="math inline">\(T\)</span>.</li>
</ul>
</div>
<div id="CI" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Confidence Intervals</h3>
<p>Back to our example now! We are clearly in need of some measure of <em>confidence</em> about our sample statistic <span class="math inline">\(\bar{x} = 168.5\)</span> before we communicate our result. It seems reasonable to inform the Director about <span class="math inline">\(\bar{x}\)</span>, but surely we also need to tell him that there was considerable <em>dispersion</em> in the data: Some people were as short as 143.98cm, while others were as tall as 189.41cm!</p>
<p>The way to proceed is to construct a <em>confidence interval</em> about the true population mean <span class="math inline">\(\mu\)</span>, based on <span class="math inline">\(\bar{x}\)</span>, which will take this uncertainty into account. We will use the <em>t</em> statistic from above. We want to have a <em>symmetric interval</em> around <span class="math inline">\(\bar{x}\)</span> which contains the true value <span class="math inline">\(\mu\)</span> with probability <span class="math inline">\(1-\alpha\)</span>. One very popular choice of <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(0.05\)</span>, hence we cover <span class="math inline">\(\mu\)</span> with 95% probability. After computing our statistic <span class="math inline">\(T\)</span> as defind in <a href="std-errors.html#eq:tscore">(6.6)</a>, this interval is defined as follows:</p>
<span class="math display" id="eq:ci">\[\begin{align}
\Pr \left(-c \leq T \leq c \right) = 1-\alpha \tag{6.7}
\end{align}\]</span>
<p>where <span class="math inline">\(c\)</span> stands for <em>critical value</em>, which we need to choose. This is illustrated in figure <a href="std-errors.html#fig:cifig">6.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:cifig"></span>
<img src="ScPoEconometrics_files/figure-html/cifig-1.png" alt="Confidence Interval Construction. The blue area is called *coverage region* which contains the true $\mu$ with probability $1-\alpha$." width="90%" />
<p class="caption">
Figure 6.3: Confidence Interval Construction. The blue area is called <em>coverage region</em> which contains the true <span class="math inline">\(\mu\)</span> with probability <span class="math inline">\(1-\alpha\)</span>.
</p>
</div>
<p>Given the symmetry of the <em>t</em> distribution it’s enough to find <span class="math inline">\(c\)</span> at the upper tail: the point above which <span class="math inline">\(\frac{\alpha}{2}\)</span> of all probability mass of the <span class="math inline">\(t_{df}\)</span> distribution comes to lie. In other words, if <span class="math inline">\(\mathcal{T}_{df}\)</span> is the CDF of the <em>t</em> distribution with <em>df</em> degrees of freedom, we find <span class="math inline">\(c\)</span> as</p>
<span class="math display" id="eq:ci1">\[\begin{align}
\mathcal{T}_{df}(c)\equiv&amp; \Pr \left( T &lt; c \right)  = 1-\frac{\alpha}{2} = 0.975 \\\tag{6.8}
c =&amp; \mathcal{T}_{df}^{-1}(\mathcal{T}_{df}(c)) = \mathcal{T}_{df}^{-1}(0.975)
\end{align}\]</span>
<p>Here <span class="math inline">\(\mathcal{T}_{df}^{-1}\)</span> stands for the <em>quantile function</em>, i.e. the inverse of the CDF. In our example with <span class="math inline">\(df = 49\)</span>, you can find thus that <span class="math inline">\(c = 2.01\)</span> by typing <code>qt(0.975,df=49)</code> into your <code>R</code> session.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> Now we only have to expand the definition of the <em>T</em> statistic from <a href="std-errors.html#eq:tscore">(6.6)</a> inside <a href="std-errors.html#eq:ci">(6.7)</a> to obtain</p>
<span class="math display" id="eq:ci2">\[\begin{align}
0.95 = 1-\alpha &amp;= \Pr \left(-c \leq T \leq c \right) \\\tag{6.9}
                &amp;= \Pr \left(-2.01 \leq \sqrt{n} \frac{\bar{x} - \mu}{s} \leq 2.01 \right) \\
                 &amp;= \Pr \left(\bar{x} -2.01 \frac{s}{\sqrt{n}} \leq \mu \leq \bar{x} + 2.01 \frac{s}{\sqrt{n}} \right) 
\end{align}\]</span>
<p>Finally, filling in our numbers for <span class="math inline">\(s\)</span> etc, this implies that a 95% confidence interval about the location of the true average height of all SciencesPo students, <span class="math inline">\(\mu\)</span>, is given by:</p>
<span class="math display">\[\begin{equation}
CI = \left[165.658 , 171.342 \right]
\end{equation}\]</span>
<p>We would tell the director that with 95% probability, the true average height of all students comes to lie within those two bounds.</p>
<p>Finally, looking back at figure <a href="std-errors.html#fig:confint">6.1</a> above, the shaded area is just the 95% confidence interval <em>about the true value <span class="math inline">\(\beta_1\)</span></em>. We would say that <em>the true regression line</em> is contained within the shaded region with 95% probability. Very similarly to our example of <span class="math inline">\(\bar{x}\)</span>, in that picture we have instead an estimate <span class="math inline">\(b_1\)</span>, with an associated standard error <span class="math inline">\(SE(b_1)\)</span>. The shaded area is called <em>confidence band</em>, and it is just plotting the confidence interval <em>for each value <span class="math inline">\(x\)</span></em> in the data. You can see how the band becomes narrower (i.e. the estimate becomes more precise) if there is more data associated to a certain <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<div id="hypothesis-testing" class="section level2">
<h2><span class="header-section-number">6.4</span> Hypothesis Testing</h2>
<p>Now know by now how the standard errors of an OLS estimate are computed, and what they stand for. We can now briefly<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> discuss a very common usage of this information, in relation to which variables we should include in our regression. There is a statistical proceedure called <em>hypothesis testing</em> which helps us to make such decisions. In <a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing">hypothesis testing</a>, we have a baseline, or <em>null</em> hypothesis <span class="math inline">\(H_0\)</span>, which we want to confront with a competing <em>alternative</em> hypthesis <span class="math inline">\(H_1\)</span>. Continuing with our example of the mean height of SciencesPo students (<span class="math inline">\(\mu\)</span>), one potential hypothesis could be</p>
<span class="math display">\[\begin{align}
H_0:&amp; \mu = 167\\
H_1:&amp; \mu \neq 167
\end{align}\]</span>
<p>Here we state that under the null hypthesis, <span class="math inline">\(\mu = 167\)</span>, and under the alternative, it’s not equal to that value. This would be called a <em>two-sided</em> test, because it tests deviations from <span class="math inline">\(H_0\)</span> below as well as above. An alternative formulation could use the <em>one-sided</em> test that</p>
<span class="math display">\[\begin{align}
H_0:&amp; \mu = 167\\
H_1:&amp; \mu &gt; 167.
\end{align}\]</span>
<p>which would mean: under the null hypothesis, the average of all ScPo students’ body height is 167cm. Under the alternative, it is larger. You can immediately see that this is very similar to confidence interval construction.</p>
<p>Suppose as above that we found <span class="math inline">\(\bar{x} = 168.5\)</span>, and that the sample standard deviation is still <span class="math inline">\(s=10\)</span>. Would you regard this as strong or weak evidence against <span class="math inline">\(H_0\)</span> and in favor of <span class="math inline">\(H_1\)</span>?</p>
<p>You should now remember what you saw when you did <code>launchApp(&quot;estimate&quot;)</code>. Look again at this app and set the slider to a sample size of <span class="math inline">\(50\)</span>, just as in our running example. You can see that the app draws one hundred (100) samples for you, locates their sample mean on the x-axis, and estimates the red density.</p>
<div class="note">
<p>
The crucial thing to note here is that, given we are working with a <strong>random sample</strong> from a population with a certain distribution of <em>height</em>, our sample statistic <span class="math inline"><span class="math inline">\(\bar{x}\)</span></span> is <strong>also a random variable</strong>. Every new set of randomly drawn students would yield a different <span class="math inline"><span class="math inline">\(\bar{x}\)</span></span>, and all of them together would follow the red density in the app. In reality we often only get to draw one single sample, and we can use knowledge about the sampling distribution to make inference.
</p>
</div>
<p><br></p>
<p>Our task is now to decide if given that particular sampling distribution, given our estimate <span class="math inline">\(\bar{x}\)</span> and given an observed sample variance <span class="math inline">\(s^2\)</span>, whether <span class="math inline">\(\bar{x} = 168.5\)</span> is <em>far away</em> from <span class="math inline">\(\bar{x} = 167\)</span>, or not. The way to proceed is by computing a <em>test statistic</em>, which is to be compared to a <em>critical value</em>: if the test statistic exceeds that value, we reject <span class="math inline">\(H_0\)</span>, otherwise we cannot. The critical value depends on the sampling distribution, and the size of the test. We talk about this next.</p>
<div id="making-errors" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Making Errors</h3>
<p>There are two types of error one can make when deploying such a test:</p>
<ol style="list-style-type: decimal">
<li>We might reject <span class="math inline">\(H_0\)</span>, when in fact it is true! Here, upon observing <span class="math inline">\(\bar{x} = 168.5\)</span> we might conclude that indeed <span class="math inline">\(\mu &gt; 167\)</span> and thus we’d reject. But we might have gotten unlucky and by chance have obtained an unusually tall sample of students. This is called <strong>type one error</strong>.</li>
<li>We might <em>fail</em> to reject <span class="math inline">\(H_0\)</span> when in fact <span class="math inline">\(H_1\)</span> is true. This is called the <strong>type two error</strong>.</li>
</ol>
<p>We design a test with a certain probability of <em>type one error</em> <span class="math inline">\(\alpha\)</span> in mind. In other words, we choose with which probability <span class="math inline">\(\alpha\)</span> we are willing to make a type one error. (Notice that the best tests also avoid making type two errors! The number <span class="math inline">\(1-\Pr(\text{type 2 error})\)</span> is called <em>power</em>, hence we prefer tests with <em>high power</em>). A typical choice for <span class="math inline">\(\alpha\)</span> is 0.05, i.e. we are willing to make a type one error with probability 5%. <span class="math inline">\(\alpha\)</span> is commonly called the <strong>level of significance</strong> or the <strong>size</strong> of a test.</p>
</div>
<div id="performing-the-test" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Performing the Test</h3>
<p>We can stick to the following cookbook procedure, which is illustrated in figure <a href="std-errors.html#fig:testfig">6.4</a>.</p>
<ol style="list-style-type: decimal">
<li>Set up hypothesis and significance level:
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H_0: \mu = 167\)</span></li>
<li><span class="math inline">\(H_1: \mu &gt; 167\)</span></li>
<li><span class="math inline">\(\alpha = 0.05\)</span></li>
</ol></li>
<li>Test Statistic and test distribution:
<ul>
<li>We don’t know the true population variance <span class="math inline">\(\sigma^2\)</span>, hence we estimate it via <span class="math inline">\(s^2\)</span> from our sample.</li>
<li>The corresponding test statistic is the <em>t-statistic</em>, which follows the <a href="https://en.wikipedia.org/wiki/Student&#39;s_t-distribution">Student’s T</a> distribution.</li>
<li>That is, our statistic is <span class="math inline">\(T=\frac{\bar{x} - \mu}{s/\sqrt{n}} \sim t_{49}\)</span>, where 49 is equal to the <em>degrees of freedom</em> in this case.</li>
</ul></li>
<li>Rejection Region: We perform a one-sided test. We said we are happy with a 5% significance level, i.e. we are looking for the <span class="math inline">\(t\)</span> value which corresponds <em>just</em> to <span class="math inline">\(1-0.05 = 0.95\)</span> mass under the pdf of the <span class="math inline">\(t\)</span> distribution. More precisely, we are looking for the <span class="math inline">\(1-0.05 = 0.95\)</span> quantile of the <span class="math inline">\(t_{50}\)</span> distribution.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> This implies a critical value <span class="math inline">\(c = 1.676\)</span>, which you can verify by typing <code>qt(0.95,df=50)</code> in <code>R</code>.</li>
<li>Calculate our test statistic: <span class="math inline">\(\frac{\bar{x} - \mu}{s/\sqrt{n}} = \frac{168.5 - 167}{10/\sqrt{50}} = 1.061\)</span></li>
<li>Decide: We find that <span class="math inline">\(1.061 &lt; 1.676\)</span>. Hence, we cannot reject <span class="math inline">\(H_0\)</span>, because we only found weak evidence against it in our sample of data.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:testfig"></span>
<img src="ScPoEconometrics_files/figure-html/testfig-1.png" alt="Cookbook Testing Proceedure. Subscripts $c$ indicate *critical value*. There are two x-axis: one for values of $\bar{x}$, and one for the corresponding $t$ statistic. The red area is the rejection area. If we observe a test statistic such that $t&gt;t_c$, we feel reassured that our $\bar{x}$ is *sufficiently far away* from the hypothesized value $\mu$, such that we feel comfortable with rejecting $H_0$. And vice versa: If our test statistic falls below $t_c$, we will not reject $H_0$" width="90%" />
<p class="caption">
Figure 6.4: Cookbook Testing Proceedure. Subscripts <span class="math inline">\(c\)</span> indicate <em>critical value</em>. There are two x-axis: one for values of <span class="math inline">\(\bar{x}\)</span>, and one for the corresponding <span class="math inline">\(t\)</span> statistic. The red area is the rejection area. If we observe a test statistic such that <span class="math inline">\(t&gt;t_c\)</span>, we feel reassured that our <span class="math inline">\(\bar{x}\)</span> is <em>sufficiently far away</em> from the hypothesized value <span class="math inline">\(\mu\)</span>, such that we feel comfortable with rejecting <span class="math inline">\(H_0\)</span>. And vice versa: If our test statistic falls below <span class="math inline">\(t_c\)</span>, we will not reject <span class="math inline">\(H_0\)</span>
</p>
</div>
</div>
<div id="testing-regression-coefficients" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Testing Regression Coefficients</h3>
<p>In Regression Analysis, we often want to test a very specific alternative hypothesis: We want to have a quick way to tell us whether a certain variable <span class="math inline">\(x_k\)</span> is <em>relevant</em> in our statistical model or not. In hypothesis testing language, that would be</p>
<span class="math display" id="eq:H0">\[\begin{align}
H_0:&amp; \beta_k = 0\\
H_1:&amp; \beta_k \neq 0.\tag{6.10}
\end{align}\]</span>
<p>Clearly, if in the <strong>true</strong> regression model we find <span class="math inline">\(\beta_k=0\)</span>, this means that <span class="math inline">\(x_k\)</span> has a zero partial effect on the outcome, hence it should be excluded from the regression. Notice that we are interested in <span class="math inline">\(\beta_k\)</span>, not in <span class="math inline">\(b_k\)</span>, which is the estimator that we compute from our sample (similarly to <span class="math inline">\(\bar{x}\)</span>, which estimates <span class="math inline">\(\mu\)</span> above).</p>
<p>As such, this is a <em>two-sided test</em>. We can again illustrate this in figure <a href="std-errors.html#fig:testfig2">6.5</a>. Notice how we now have two rejection areas.</p>
<div class="figure" style="text-align: center"><span id="fig:testfig2"></span>
<img src="ScPoEconometrics_files/figure-html/testfig2-1.png" alt="Testing whether coefficient $b_k$ is *statistically significantly different* from zero. Now we have two red rejection areas. We relabel critical values with a superscript here. If we observe a test statistic falling in either red region, we reject, else we do not. Notice that the true value under $H_0$ is $\beta_k=0$. " width="90%" />
<p class="caption">
Figure 6.5: Testing whether coefficient <span class="math inline">\(b_k\)</span> is <em>statistically significantly different</em> from zero. Now we have two red rejection areas. We relabel critical values with a superscript here. If we observe a test statistic falling in either red region, we reject, else we do not. Notice that the true value under <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\beta_k=0\)</span>.
</p>
</div>
<p>The relevant test statistic for a regression coefficient is again the <em>t</em> distribution. In fact, this particular test is so important that all statistical packages report the <em>t</em> statistic corresponding to <a href="std-errors.html#eq:H0">(6.10)</a> automatically. Let’s look at an example:</p>
<pre><code>#OUT&gt; 
#OUT&gt; Call:
#OUT&gt; lm(formula = mpg ~ wt + hp + drat, data = mtcars)
#OUT&gt; 
#OUT&gt; Residuals:
#OUT&gt;     Min      1Q  Median      3Q     Max 
#OUT&gt; -3.3598 -1.8374 -0.5099  0.9681  5.7078 
#OUT&gt; 
#OUT&gt; Coefficients:
#OUT&gt;              Estimate Std. Error t value Pr(&gt;|t|)    
#OUT&gt; (Intercept) 29.394934   6.156303   4.775 5.13e-05 ***
#OUT&gt; wt          -3.227954   0.796398  -4.053 0.000364 ***
#OUT&gt; hp          -0.032230   0.008925  -3.611 0.001178 ** 
#OUT&gt; drat         1.615049   1.226983   1.316 0.198755    
#OUT&gt; ---
#OUT&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#OUT&gt; 
#OUT&gt; Residual standard error: 2.561 on 28 degrees of freedom
#OUT&gt; Multiple R-squared:  0.8369,  Adjusted R-squared:  0.8194 
#OUT&gt; F-statistic: 47.88 on 3 and 28 DF,  p-value: 3.768e-11</code></pre>
<p>The column <code>t value</code> is just <code>Estimate</code> divided by <code>Std. Error</code>. That is, <code>R</code> reports in the column <code>t value</code> the following number for us:</p>
<span class="math display" id="eq:tstat">\[\begin{equation}
\text{t value} = \frac{b_k-0}{s_k}  \tag{6.11}
\end{equation}\]</span>
<p>where <span class="math inline">\(s_k\)</span> is the estimated standard error as introduced in <a href="std-errors.html#se-theory">6.2.2</a>, and where we test <span class="math inline">\(H_0:\beta_k = 0\)</span>. Notice that this particular <em>t</em> statistic is different from our previous formulation in <a href="std-errors.html#eq:tscore">(6.6)</a>: we don’t have to scale by <span class="math inline">\(\sqrt{n}\)</span>! This is so because <code>R</code> and other statistical software assumes the <em>normal</em> linear regression model (see <a href="std-errors.html#class-reg">6.2</a>). Normality of the regression error <span class="math inline">\(\varepsilon\)</span> implies that the <em>t</em> statistic looks like in <a href="std-errors.html#eq:tstat">(6.11)</a>.</p>
<p>We have to choose a critical value for this test. Many people automatically choose the 0.975 quantile of the standard normal distribution, <code>qnorm(0.975)</code>, 1.96 in this case. This is fine for sample sizes greater than 100, say. In this regression, we only have 28 degrees of freedom, so we better choose the critical value from the <em>t</em> distribution as above. We get <span class="math inline">\(t_{down} = -2.048\)</span> and <span class="math inline">\(t_{up} = 2.048\)</span> as critical values. Let’s test whether the coefficient on <code>wt</code> is statistically different from zero:</p>
<span class="math display" id="eq:mtcarswt">\[\begin{align}
H_0:&amp; \beta_{wt} = 0\\
H_1:&amp; \beta_{wt} \neq 0 \tag{6.12}
\end{align}\]</span>
<p>We just take the <code>t value</code> entry, and see whether it lies above or below either critical value: Indeed, we see that <span class="math inline">\(-4.053 &lt; -2.048\)</span>, and we are happy to reject <span class="math inline">\(H_0\)</span>.</p>
<p>On the other hand, when testing for statistical significance of <code>drat</code> that does not seem to be the case:</p>
<span class="math display" id="eq:mtcarsdrat">\[\begin{align}
H_0:&amp; \beta_{drat} = 0\\
H_1:&amp; \beta_{drat} \neq 0 \tag{6.13}
\end{align}\]</span>
<p>Here we find that <span class="math inline">\(1.316 \in [-2.048,2.048]\)</span>, hence it does not lie in any rejection region, and we can <em>not</em> reject <span class="math inline">\(H_0\)</span>. We would say that <em>coefficient <span class="math inline">\(\beta_{drat}\)</span> is not statistically significant at the 5% level</em>. As such, we should not include it in our regression.</p>
</div>
<div id="p-values-and-stars" class="section level3">
<h3><span class="header-section-number">6.4.4</span> P-Values and Stars</h3>
<p><code>R</code> also reports two additional columns in its regression output. The so-called <em>p-value</em> in column <code>Pr(&gt;|t|)</code> and a column with stars. P-values are an improvement over the dichotomy introduced in the standard reject/accept framework above. We never know if we <em>narrowly</em> rejected a <span class="math inline">\(H_0\)</span>, or not. The p-value is defined as the particular level of significance <span class="math inline">\(\alpha^*\)</span>, up to which <em>all</em> <span class="math inline">\(H_0\)</span>’s would be rejected. If this is a very small number, we have overwhelming support to reject the null. If, on the contrary, <span class="math inline">\(\alpha^*\)</span> turns out to be rather large, we only found weak evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>We define the p-value as the sum of rejection areas for a given test statistic <span class="math inline">\(T^*\)</span>. Notice that the symmetry of the <span class="math inline">\(t\)</span> distribution implies that we multiply by two each of the two tail probabilities:</p>
<span class="math display">\[\begin{align}
\alpha^* = 2 \Pr(t &gt; |T^*|) 
\end{align}\]</span>
<p>The stars in the final column are a visualization of this information. They show a quick summary of the magnitude of each p-value. Commonly, <code>***</code> means an extremely small reference significance level <span class="math inline">\(\alpha^*=0\)</span> (almost zero), <code>**</code> means <span class="math inline">\(\alpha^*=0.001\)</span>, etc. In that case, up to a significance level of 0.1%, all <span class="math inline">\(H_0\)</span> would be rejected. You clearly see that all columns <code>Std. Error</code>, <code>t value</code> and <code>Pr(&gt;|t|)</code> give a different type of the same information.</p>
</div>
</div>
<div id="whats-in-my-model-and-what-is-not" class="section level2">
<h2><span class="header-section-number">6.5</span> What’s in my model? (And what is not?)</h2>
<p>We want to revisit the underlying assumptions of the classical model outlined in <a href="std-errors.html#class-reg">6.2</a>. Right now we to talk a bit more about assumption number 2 of the above definition in <a href="std-errors.html#class-reg">6.2</a>. It said this:</p>
<div class="warning">
<p>
The mean of the residuals conditional on <span class="math inline"><span class="math inline">\(x\)</span></span> should be zero, <span class="math inline"><span class="math inline">\(E[\varepsilon|x] = 0\)</span></span>. This means that <span class="math inline"><span class="math inline">\(Cov(\varepsilon,x) = 0\)</span></span>, i.e. that the errors and our explanatory variable(s) should be <em>uncorrelated</em>. We want <span class="math inline"><span class="math inline">\(x\)</span></span> to be <strong>strictly exogenous</strong> to the model.
</p>
</div>
<p><br> Great. But what does this <em>mean</em>? How could <span class="math inline">\(x\)</span> be correlated with something we don’t even observe?! Good questions - let’s try with an example.</p>
<p>Imagine that we assume that</p>
<span class="math display" id="eq:DGP-h">\[\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \tag{6.14}
\end{equation}\]</span>
<p>represents the DGP of impact the sales price of houses (<span class="math inline">\(y\)</span>) as a function of number of bathrooms (<span class="math inline">\(x\)</span>). We run OLS as</p>
<p><span class="math display">\[
y_i = b_0 + b_1 x_i + e_i 
\]</span> You find a positive impact of bathrooms on houses:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Housing, <span class="dt">package=</span><span class="st">&quot;Ecdat&quot;</span>)
hlm =<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>bathrms, <span class="dt">data =</span> Housing)
<span class="kw">summary</span>(hlm)</code></pre></div>
<pre><code>#OUT&gt; 
#OUT&gt; Call:
#OUT&gt; lm(formula = price ~ bathrms, data = Housing)
#OUT&gt; 
#OUT&gt; Residuals:
#OUT&gt;    Min     1Q Median     3Q    Max 
#OUT&gt; -77225 -15271  -2510  11704 102729 
#OUT&gt; 
#OUT&gt; Coefficients:
#OUT&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#OUT&gt; (Intercept)    32794       2694   12.17   &lt;2e-16 ***
#OUT&gt; bathrms        27477       1952   14.08   &lt;2e-16 ***
#OUT&gt; ---
#OUT&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#OUT&gt; 
#OUT&gt; Residual standard error: 22880 on 544 degrees of freedom
#OUT&gt; Multiple R-squared:  0.267,   Adjusted R-squared:  0.2657 
#OUT&gt; F-statistic: 198.2 on 1 and 544 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In fact, from this you conclude that each additional bathroom increases the sales price of a house by 27477 dollars. Let’s see if our assumption <span class="math inline">\(E[\varepsilon|x] = 0\)</span> is satisfied:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="co"># add residuals to the data</span>
Housing<span class="op">$</span>resid &lt;-<span class="st"> </span><span class="kw">resid</span>(hlm)
Housing <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(bathrms) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mean_of_resid=</span><span class="kw">mean</span>(resid))</code></pre></div>
<pre><code>#OUT&gt; # A tibble: 4 x 2
#OUT&gt;   bathrms mean_of_resid
#OUT&gt;     &lt;dbl&gt;         &lt;dbl&gt;
#OUT&gt; 1       1         -118.
#OUT&gt; 2       2          955.
#OUT&gt; 3       3       -11195.
#OUT&gt; 4       4        32298.</code></pre>
<p>Oh, that doesn’t look good. Even though the unconditional mean <span class="math inline">\(E[e] = 0\)</span> is <em>very</em> close to zero (type <code>mean(resid(hlm))</code>!), this doesn’t seem to hold at all by categories of <span class="math inline">\(x\)</span>. This indicates that there is something in the error term <span class="math inline">\(e\)</span> which is <em>correlated</em> with <code>bathrms</code>. Going back to our discussion about <em>ceteris paribus</em> in section <a href="multiple-reg.html#ceteris">4.1</a>, we stated that the interpretation of our OLS slope estimate is that</p>
<div class="tip">
<p>
Keeping everything else fixed at the current value, what is the impact of <span class="math inline"><span class="math inline">\(x\)</span></span> on <span class="math inline"><span class="math inline">\(y\)</span></span>? <em>Everything</em> also includes things in <span class="math inline"><span class="math inline">\(\varepsilon\)</span></span> (and, hence, <span class="math inline"><span class="math inline">\(e\)</span></span>)!
</p>
</div>
<p><br> It looks like our DGP in <a href="std-errors.html#eq:DGP-h">(6.14)</a> is the <em>wrong model</em>. Suppose instead, that in reality sales prices are generated like this:</p>
<span class="math display" id="eq:DGP-h2">\[\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \varepsilon_i \tag{6.15}
\end{equation}\]</span>
<p>This would now mean that by running our regression, informed by the wrong DGP, what we estimate is in fact this: <span class="math display">\[
y_i = b_0 + b_1 x_i + (b_2 z_i + e_i)  = b_0 + b_1 x_i + u_i.
\]</span> This is to say that by <em>omitting</em> variable <span class="math inline">\(z\)</span>, we relegate it to a new error term, here called <span class="math inline">\(u_i = b_2 z_i + e_i\)</span>. Our assumption above states that <em>all regressors need to be uncorrelated with the error term</em> - so, if <span class="math inline">\(Corr(x,z)\neq 0\)</span>, we have a problem. Let’s take this idea to our running example.</p>
<div id="omitted-variable-bias" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Omitted Variable Bias</h3>
<p>What we are discussing here is called <em>Omitted Variable Bias</em>. There is a variable which we omitted from our regression, i.e. we forgot to include it. It is often difficult to find out what that variable could be, and you can go a long way by just reasoning about the data-generating process. In other words, do you think it’s <em>reasonable</em> that price be determined by the number of bathrooms only? Or could there be another variable, omitted from our model, that is important to explain prices, and at the same time correlated with <code>bathrms</code>?</p>
<p>Let’s try with <code>lotsize</code>, i.e. the size of the area on which the house stands. Intuitively, larger lots should command a higher price; At the same time, however, larger lots imply more space, hence, you can also have more bathrooms! Let’s check this out:</p>
<pre><code>#OUT&gt; 
#OUT&gt; Call:
#OUT&gt; lm(formula = price ~ bathrms + lotsize, data = Housing)
#OUT&gt; 
#OUT&gt; Residuals:
#OUT&gt;    Min     1Q Median     3Q    Max 
#OUT&gt; -60752 -12532  -1674  10514  92931 
#OUT&gt; 
#OUT&gt; Coefficients:
#OUT&gt;              Estimate Std. Error t value Pr(&gt;|t|)    
#OUT&gt; (Intercept) 1.008e+04  2.810e+03   3.588 0.000364 ***
#OUT&gt; bathrms     2.281e+04  1.703e+03  13.397  &lt; 2e-16 ***
#OUT&gt; lotsize     5.575e+00  3.944e-01  14.136  &lt; 2e-16 ***
#OUT&gt; ---
#OUT&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#OUT&gt; 
#OUT&gt; Residual standard error: 19580 on 543 degrees of freedom
#OUT&gt; Multiple R-squared:  0.4642,  Adjusted R-squared:  0.4622 
#OUT&gt; F-statistic: 235.2 on 2 and 543 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here we see that the estimate for the effect of an additional bathroom <em>decreased</em> from 27477 to 22811.5 by almost 5000 dollars! Well that’s the problem then. We said above that one more bathroom is worth 27477 dollars - if <strong>nothing else changes</strong>! But that doesn’t seem to hold, because we have seen that as we increase <code>bathrms</code> from <code>1</code> to <code>2</code>, the mean of the resulting residuals changes quite a bit. So there <strong>is something in <span class="math inline">\(\varepsilon\)</span> which does change</strong>, hence, our conclusion that one more bathroom is worth 27477 dollars is in fact <em>invalid</em>!</p>
<p>The way in which <code>bathrms</code> and <code>lotsize</code> are correlated is important here, so let’s investigate that:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-159"></span>
<img src="ScPoEconometrics_files/figure-html/unnamed-chunk-159-1.png" alt="Distribution of `lotsize` by `bathrms`" width="672" />
<p class="caption">
Figure 6.6: Distribution of <code>lotsize</code> by <code>bathrms</code>
</p>
</div>
<p>This shows that lotsize and the number of bathrooms is indeed positively related. Larger lot of the house, more bathrooms. This leads to a general result:</p>
<div class="note">
<p>
<strong>Direction of Omitted Variable Bias</strong>
</p>
<p>
If there is an omitted variable <span class="math inline"><span class="math inline">\(z\)</span></span> that is <em>positively</em> correlated with our explanatory variable <span class="math inline"><span class="math inline">\(x\)</span></span>, then our estimate of effect of <span class="math inline"><span class="math inline">\(x\)</span></span> on <span class="math inline"><span class="math inline">\(y\)</span></span> will be too large (or, <em>biased upwards</em>). The correlation between <span class="math inline"><span class="math inline">\(x\)</span></span> and <span class="math inline"><span class="math inline">\(z\)</span></span> means that we attribute part of the impact of <span class="math inline"><span class="math inline">\(z\)</span></span> on <span class="math inline"><span class="math inline">\(y\)</span></span> mistakenly to <span class="math inline"><span class="math inline">\(x\)</span></span>! And, of course, vice versa for <em>negatively</em> correlated omitted variables.
</p>
</div>
<p><br></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>You often will see <span class="math inline">\(c=1.96\)</span>, which comes from the fact that one relies on the <em>t</em> distribution converging to the normal distribution with large <span class="math inline">\(n\)</span>. Type <code>qnorm(0.975)</code> to confirm!<a href="std-errors.html#fnref5">↩</a></p></li>
<li id="fn6"><p>We will not go into great detail here. Please refer back to your statistics course from last spring semester (chapters 8 and 9), or the short note I <a href="images/hypothesis.pdf">wrote while ago</a><a href="std-errors.html#fnref6">↩</a></p></li>
<li id="fn7"><p>See the previous footnote for an explanation of this!<a href="std-errors.html#fnref7">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="categorical-vars.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="quantreg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ScPoEcon/ScPoEconometrics/edit/master/06-StdErrors.Rmd",
"text": "Edit"
},
"download": ["ScPoEconometrics.pdf", "ScPoEconometrics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
