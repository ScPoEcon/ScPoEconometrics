---
title: "ScPoEconometrics4"
author: "Florian Oswald"
date: "`r Sys.Date()`"
output: 
    ioslides_presentation:
        highlight: textmate
        widescreen: true
        logo: ../../images/ScPo.png
        self_contained: false

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Multiple Regression

* Instead of one regressor, we can have *multiple* ones.
* For example, how do horse power (hp) and weight (wt) relate to miles per gallon?
  $$
  mpg_i = b_0 + b_1 hp_i + b_2 wt_i + e_i
  $$
  
* Well, how?


## Multiple Regression Visualized

* Let's have a look at [the 3D figure in the book](https://scpoecon.github.io/ScPoEconometrics/multiple-reg.html)

## *Multiple* Regression: $K$ regressors

* *Multiple* means there can be more than 2 regressors as well.
* In fact, there could be $K$ of them.
$$
\begin{aligned}
\widehat{y}_i &= b_0 + b_1 x_{1i} +   b_2 x_{2i} + \dots + b_K x_{Ki}\\
e_i &= y_i - \widehat{y}_i 
\end{aligned}
$$
* The *partial effect* of regressor $j$ is the partial derivative
$$
\frac{\partial y}{\partial x_j} = b_j
$$

# Ceteris Paribus

## All Else Equal (Ceteris Paribus)

* How does $mpg$ change if we change $hp$?
* **This question only makes sense if $wt$ does not change.**
* *Holding the others constant*: partial derivative.
$$
\frac{\partial mpg_i}{\partial hp_i} = b_1 
$$
* Do this!
    ```{r,eval=FALSE,echo=TRUE}
    lm(formula = mpg ~ wt + hp, data = mtcars)
    ```

## Let's find the best fit in 3D!

* We have done this before in two dimensions.
* Now, we have to choose not only $b_0,b_1$, but also $b_2$!
* Let's do it!

```{r,eval=FALSE}
launchApp("reg_multivariate")
```

## Multicolinearity

> * Multiple regression requires that variables are **linearly independent**.
> * Imagine we had this instead to our initial equation:
    $$
    mpg_i = b_0 + b_1 wt_i + b_2 wtplus1_i + e_i
    $$
    where `wtplus1 = wt + 1`.
> * `wtplus1` wouldn't add any information - it's redundant
> * If we only have one `x`, this translates into $Var(x)\neq0$.
> * Gives rise to the *rank condition*, i.e. that $N\geq K+1$
> * The higher the *linear dependence* amongst our explanatory variable, the less information we can extract from them: Our estimtes **become less precise** as a result!

## Multicolinearity App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("multicolinearity")
```


## California Test Scores 2

* Let's go back to the `Caschool` dataset.
* Could `testscr` also depend on average income in the school area?
    $$
    \text{testscr}_i = b_0 + b_1  \text{str}_i + b_2  \text{avginc}_i + e_i
    $$
* We simply add `avginc` to our `formula`:

```{r,echo=TRUE,eval=FALSE}
library("Ecdat") # reload the data
fit_multivariate <- lm(formula = "testscr ~ str + avginc", data = Caschool)
summary(fit_multivariate)
```

## Visualizing California Test Scores 2

* Let's again look at the [plot](https://scpoecon.github.io/ScPoEconometrics/multiple-reg.html#california-test-scores-2)

# Interactions

## Interactions

* We can relax the ceteris paribus assumption. 
* If we want to allow the effect of one variable to depend on the *value* of another one, we can do this.
* Is the effect of `str` *dependent* on `avginc`? Is the effect **stronger** in richer areas?
* Go back to the [book](https://scpoecon.github.io/ScPoEconometrics/multiple-reg.html#california-test-scores-2)


# Tutorial - `CPS1988` 

## Tutorial Time!

```{r,eval=FALSE}
library(ScPoEconometrics)
runTutorial("lm-example")
```


