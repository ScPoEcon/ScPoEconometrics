ScPoEconometrics 3
========================================================
author: Florian Oswald
date: `r Sys.Date()`
autosize: true
css: ../style.css

Data On Cars
============

* Suppose we had data on car `speed` and stopping `dist` (ance):

```{r}
head(cars)
```

* How are `speed` and `dist` related?

Scatterplot of Cars
===================
incremental:true

```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=8}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
```

* What's a reasonable **summary** of this relationship?

A Line throught the Scatterplot of Cars
===================
incremental:true

```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=8}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 60,b = 0,lw=3)
```

* *A line*. Great. But **which** line? This one?
* That's a *flat* line. But `dist` is increasing. :-(


A Line throught the Scatterplot of Cars
===================
incremental:true

```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=8}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 0,b = 5,lw=3)
```

* **That** one?
* Slightly better. Has a *slope* and an *intercept*.

Writing Down A Line
=============
incremental:true

* We observe $(y_i,x_i)$ in the data.
* Here is a formula that describes their relationship:
$$
y_i = b_0 + b_1 x_i + e_i
$$
* How to choose $b_0$ and $b_1$ s.t. this is **best** line?

Prediction and Errors
===============
incremental:true

* We need to denote the *output* of the line.
* We call the value $\widehat{y}_i$ the *predicted value*, after we choose values for $b_0,b_1$ and $x$.
$$
\widehat{y}_i = b_0 + b_1 x_i
$$
* In general, we **expect** to make errors!

Towards The Best Line
======================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  meandev = y - y_bar
  data.frame(x, y, y_hat, y_bar, error, meandev)
}

plot_total_dev = function(reg_data,title=NULL) {
  if (is.null(title)){
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4, col = "grey")
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 0, blue = 1, alpha = 0.5), border = NA)
  } else {
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey",main=title,ylim=c(-2,10.5))
     axis(side=2,at=seq(-2,10,by=2))
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 0, green = 0, blue = 1, alpha = 0.5), border = NA)
  }
  # arrows(reg_data$x, reg_data$y_bar,
  #        reg_data$x, reg_data$y,
  #        col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 3,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 4, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
}

plot_unexp_SSR = function(reg_data,asp=1,title=NULL) {
  if (is.null(title)){
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA),asp=asp)
      abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
  } else {
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA),asp=asp,main=title)
    axis(side=2,at=seq(-2,10,by=2))
      abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
  }
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)",
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="The best line and its errors",fig.align="center",fig.height=5,fig.width=8}
plot_unexp_dev(plot_data)
```

* Red Arrows are *errors* or *residuals* (often $u$ or $e$)


App Time!
=========

```{r,eval=FALSE}
library(ScPoEconometrics) # load our library
launchApp('reg_simple_arrows')
aboutApp('reg_simple_arrows')
```

Writing Down The Best Line
=============
incremental:true

* choose $(b_0,b_1)$ s.t. the sum $e_1^2 + \dots + e_N^2$ is **as small as possible**
* $e_1^2 + \dots + e_N^2$ is the *sum of squared residuals*, or SSR.
* Wait a moment... Why *squared* residuals?!

Minimizing Squared Residuals
=============
incremental:true

* In previous plot, errors of different sign ($+/-$) cancel out!
* The line with minimal SSR need not be a *very good* summary of our data.
* Squaring each $e_i$ solves that issue as $e_i^2 \geq 0, \forall i$

Best Line and Squared Errors
=============
incremental:true

```{r line-squares, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
plot_unexp_SSR(plot_data)
```

App Time!
=========

```{r,eval=FALSE}
launchApp('reg_simple')
aboutApp('reg_simple')
```

Ordinary Least Squares (OLS)
=================
incremental:true

* OLS estimates the best line for us.
* In our single regressor case, there is a simple formula for the slope:
  $$
  b_1 = \frac{cov(x,y)}{var(x)}
  $$
* and for the intercept
  $$
  b_0 = \bar{y} - b_1\bar{x}
  $$

App Time!
=========

How does OLS actually perform the minimization problem?

```{r,eval=FALSE}
launchApp('SSR_cone')
aboutApp('SSR_cone')  # after
```

App Time!
=========

Let's do some more OLS!

```{r,eval=FALSE}
launchApp('reg_full')
aboutApp('reg_full')  # after
```

Common Restrictions on OLS
==========
incremental:true

* There are some common *flavors* of OLS.
* We will go through some of them.
* E.g. what happens without an intercept?
* Or, what happens if we *demean* both $y$ and $x$?

OLS without any Regressor
===============
incremental: true

* Our line is flat at level $b_0$:
  $$
  y = b_0
  $$
* Our optimization problem is now
  $$
  b_0 = \arg\min_{\text{int}} \sum_{i=1}^N \left[y_i - \text{int}\right]^2,
  $$
* With solution
  $$
  b_0 = \frac{1}{N} \sum_{i=1}^N y_i = \overline{y}.
  $$


Regression without Intercept
=============
incremental: true

* Now we have a line anchored at the origin.
* The OLS slope estimate becomes
    $$
    \begin{align}
    b_1 &= \arg\min_{\text{slope}} \sum_{i=1}^N \left[y_i - \text{slope } x_i \right]^2\\
    \mapsto b_1 &= \frac{\frac{1}{N}\sum_{i=1}^N x_i y_i}{\frac{1}{N}\sum_{i=1}^N x_i^2} = \frac{\bar{x}     \bar{y}}{\overline{x^2}}
    \end{align}
    $$

App: Regressions w/o Slope or Intercept
=========

```{r,eval=FALSE}
launchApp('reg_constrained')
```

Centering a Regression
============
incremental: true

* *centering* or *demeaning* means to substract the mean.
* We get $\widetilde{y}_i = y_i - \bar{y}$.
* Let's run a regression *without* intercept as above using $\widetilde{y}_i,\widetilde{x}_i$
* We get
    $$
    \begin{align}
    b_1 &= \frac{\frac{1}{N}\sum_{i=1}^N \widetilde{x}_i \widetilde{y}_i}{\frac{1}{N}\sum_{i=1}^N \widetilde{x}_i^2}\\
        &= \frac{cov(x,y)}{var(x)}
    \end{align}
    $$

App: demeaned regression
=========

```{r,eval=FALSE}
launchApp('demeaned_reg')
```

Standardizing a Regression
============
incremental: true

* To standardize $z$ means to do $\breve{z}=\frac{z-\bar{z}}{sd(z)}$
* I.e. substract the variable's mean and divide by its standard deviation.
* Proceed as above, but with $\breve{y}_i,\breve{x}_i$
* We get
    $$
    \begin{align}
    b_1 &= \frac{\frac{1}{N}\sum_{i=1}^N \breve{x}_i \breve{y}_i}{\frac{1}{N}\sum_{i=1}^N \breve{x}_i^2}\\
        &= \frac{Cov(x,y)}{\sigma_x \sigma_y}=Corr(x,y)
    \end{align}
    $$

App: Standardized regression
=========

```{r,eval=FALSE}
launchApp('reg_standardized')
```


Predictions and Residuals
=====================
incremental: true

1. The error is $e_i = y_i - \widehat{y}_i$
1. The average of $\widehat{y}_i$ is equal to $\bar{y}$.
    $$
    \begin{align}
    \frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\
    &= b_0 + b_1  \bar{x}  = \bar{y}\\
    \end{align}
    $$
1. Then,
    $$\frac{1}{N} \sum_{i=1}^N e_i = \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i = 0$$
    i.e. the average of errors is zero.



Prediction and Errors are Orthogonal
======================================
class: small-code
incremental:true

* This is the data of the above plots (arrows and squares):
```{r,echo=FALSE}
ss = subset(plot_data,select=c(x,y,y_hat,error))
round(ss,2)
```

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.
2. The average of the errors should be zero.
3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

Prediction and Errors are Orthogonal
======================================
class: small-code
incremental:true

```{r}
# 1.
all.equal(mean(ss$error), 0)
# 2.
all.equal(mean(ss$y_hat), mean(ss$y))
# 3.
all.equal(cov(ss$error,ss$y_hat), 0)
```

Linear Statistics
=====================
incremental: true

* It's important to keep in mind that Var, Cov, Corr and Regression measure **linear relationships** between two variables.
* Two datasets with *identical* correlations could look *vastly* different.
* They would have the same regression line.
* Same correlation coefficient.
* Is that even possible?

Linear Statistics: Anscombe
=====================
incremental: true

* Francis Anscombe (1973) comes up with 4 datasets with identical stats. But look!

    ```{r,echo=FALSE}
    ##-- now some "magic" to do the 4 regressions in a loop:
    ff <- y ~ x
    mods <- setNames(as.list(1:4), paste0("lm", 1:4))
    for(i in 1:4) {
      ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
      ## or   ff[[2]] <- as.name(paste0("y", i))
      ##      ff[[3]] <- as.name(paste0("x", i))
      mods[[i]] <- lmi <- lm(ff, data = anscombe)
    }

    op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
    for(i in 1:4) {
      ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
      plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
           xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
      abline(mods[[i]], col = "blue")
    }
    par(op)
    ```

Dinosaurs in your Data?
=====================
incremental: true

* So, be wary of only looking a linear summary stats.
* Also look at plots.
* Dinosaurs?
    ```{r,eval=FALSE}
    launchApp("datasaurus")
    aboutApp("datasaurus")
    ```


Nonlinear Relationships in Data?
=====================
incremental: true

* We can accomodate non-linear relationships in regressions.
* We'd just add a higher order term like this:
    $$
    y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
    $$
* This is *multiple regression* (next chapter!)

Nonlinear Relationships in Data?
=====================
incremental:true

* For example, suppose we had this data and fit the above regression:
    ```{r non-line-cars-ols2,echo=FALSE,fig.align="center",fig.cap="Better line with non-linear data!",echo=FALSE}
    l1 = lm(mpg~hp+I(hp^2),data=mtcars)
    newdata=data.frame(hp=seq(from=min(mtcars$hp),to=max(mtcars$hp),length.out=100))
    newdata$y = predict(l1,newdata=newdata)
    plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y")
    lines(newdata$hp,newdata$y,lw=2)
    ```

Analysis of Variance
=====================
incremental: true

* Remember that $y_i = \widehat{y}_i + e_i$.
* We have the following decomposition:
    $$
    \begin{align}
    Var(y) &= Var(\widehat{y} + e)\\
     &= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\
     &= Var(\widehat{y}) + Var(e)
    \end{align}
    $$
* Because: $Cov(\hat{y},e)=0$
* Total variation (SST) = Model explained (SSE) + Unexplained (SSR)




Assessing the Goodness of Fit
=====================
incremental: true

* The $R^2$ measures how good the model fits the data.
* $R^2=1$ is very good, $R^2=0$ is very poorly.
    $$
    R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
    $$
* Small $R^2$ doesn't mean it's a useless model!



An Example - California Test Scores
=====================

* Let's look at [the worked example](https://scpoecon.github.io/ScPoEconometrics/linreg.html#lm-example1) in the book!


Rescaling Regressions
=====================
incremental: true

* Suppose outcome $y$ is *income in Euros*
* $x$ be years of schooling
  $$
  y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
  $$
* Assume that $\beta_1 = 2000$, s.t. each additional year of schooling gives 2000 Euros more income.
* What is $\beta_1$ if we measure $y$ in *thousands of euros* instead?

Rescaling Regressions
======================
incremental: true

* The result depends on whether we rescale $y$, $x$ or both.
* If we have rescale $x$ by $c$ and $y$ by $d$, one can show that
    $$
    \begin{align}
    b_1' &= \frac{Cov(dx,cy)}{Var(dx)}\\
         &= \frac{d}{c} b
         \end{align}
    $$
* and the intercept
    $$
    \begin{align}
    b_0' &= \mathbb{E}[d\cdot Y] - \frac{Cov(cX, dY)}{Var(cX)} \mathbb{E}[c\cdot X] \\
    &= d b_0
    \end{align}
    $$

App: Rescaling Regressors
=========

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp('Rescale')
```


Tutorial: Rescaling Regressors
=========

```{r,eval=FALSE}
library(ScPoEconometrics)
runTutorial('rescaling')
```

