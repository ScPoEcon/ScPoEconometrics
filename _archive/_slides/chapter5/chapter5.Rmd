---
title: "ScPoEconometrics5"
author: "Florian Oswald"
date: "`r Sys.Date()`"
output: 
    ioslides_presentation:
        highlight: textmate
        widescreen: true
        logo: ../../images/ScPo.png
        self_contained: false
---

<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Categorical Variables

> * We have seen different data types in the first session.
> * One of them was `factor`, representing **categorical** data:
> * A person is *male* or *female*
> * A plane is *passenger*, *cargo* or *military*
> * Some good is produced in *Spain*, *France*, *China* or *UK*.

## Binary/Boolean/Dummy 

* A *dummy* variable is either `TRUE` or `FALSE` (or `0` or `1`).
* We use dummies to mark **category membership**: if member, then `TRUE`.
* for example, 
  $$
  \begin{aligned}
\text{is.male}_i &=  \begin{cases}
                    1 & \text{if }i\text{ is male} \\
                    0 & \text{if }i\text{ is not male}. \\
                 \end{cases} \\
  \end{aligned}
  $$
* Notice that whether `0` corresponds to `TRUE` or `FALSE` is up to you. Just be consistent!

# Single Binary Regressor

## Dummy Variables

> * We defined `is.male`. Let's define its pendant.
> * That is, for females, 
  $$
  \begin{aligned}
\text{is.female}_i &=  \begin{cases}
                    1 & \text{if }i\text{ is female} \\
                    0 & \text{if }i\text{ is not female} \\
                 \end{cases} \\
  \end{aligned}
  $$
> * let's all create this dataset:

```{r,echo=TRUE}
df1 = data.frame(income=c(3000,5000,7500,3500),
             sex=c("male","female","male","female"))
```

## Falling into The Dummy Variable Trap

* Let's run regression $y = b_0 + b_1 is.male + b_2 is.female$
* First, we create those dummy variables:

```{r,echo=TRUE}
df1$is.male = df1$sex == "male"
df1$is.female = df1$sex == "female"
```

* and then let's run this:

```{r,eval=FALSE}
lm(income ~ is.male + is.female,df1)
```

* What do you see? `r emo::ji("thinking")`

## The Trap: Multicolinearity

```{r,warning=FALSE,message=FALSE}
df1$linear_comb = df1$is.male + df1$is.female
df1
```

* Oops. `is.male + is.female` is **always** equal `1`!
* In other words, `is.male = 1 - is.female`. A **perfect colinearity**!
* Multiple regression fails. `r emo::ji("angry")`

## Drop One Category

* Notice: Inclusion of both dummies doesn't add anything
* If someone is `male` they are *not* `female`.
* So we **drop one of the categories**. Only do $y = b_0 + b_1 is.female$

```{r}
lm1 = lm(income ~ is.female,df1)
lm1
```

## Interpretation of Dummies

* We have excluded `is.male`.
* So what's the effect of being `male` now?
  * Well, *male* means `is.female = 0`. So `male` is **subsumed in the intercept**!
  * At `is.female = 0`, i.e. $\widehat{y} = b_0 + b_1 \cdot 0=$ `r coef(lm1)[1]`
* Coefficient on `is.female` is $b_1=$ `r coef(lm1)[2]`. It measures the *difference in intercept from being female*.
  * That means: $\widehat{y} = b_0 + b_1 \cdot 1=$ `r sum(coef(lm1)[1:2])`
  
## Our Dataset in a Picture

```{r x-zero,echo=FALSE,fig.align='center',fig.asp=0.9,fig.width=6}
a <- coef(lm1)[1]
b <- coef(lm1)[2]
x = df1$is.female
y = df1$income
dta = df1

# plot
expr <- function(x) a + b*x
errors <- (a + b*x) - y

plot(x,y, type = "p",pch = 21, col = "blue", bg = "royalblue",frame.plot = TRUE,cex=1.2,xlim=c(-.2, 1.7), ylim = c(min(y)-.1, max(y)+.1),xlab="is.female",ylab="income",xaxt="n")
axis(side = 1,c(0,1))
```

## Regression connects Conditional Means!

```{r x-zero-one,echo=FALSE,fig.align='center',fig.asp=0.9,fig.width=6}
plot(x,y, type = "p",pch = 21, col = "blue", bg = "royalblue",frame.plot = TRUE,cex=1.2,xlim=c(-.2, 1.7), ylim = c(min(y)-.1, max(y)+.1),xlab="is.female",ylab="income",xaxt="n")
axis(side = 1,c(0,1))

points(0, mean(dta[dta$is.female == 0, "income"]), col = 'orange',
       cex = 3, pch = 15)
text(0.35, mean(dta[dta$is.female == 0, "income"]), "E[Y | is.female = 0]", pos = 3)

points(1, mean(dta[dta$is.female == 1, "income"]), col = 'orange',
       cex = 3, pch = 15)
text(0.95, mean(dta[dta$is.female == 1, "income"]), "E[Y | is.female = 1]", pos = 2)
curve(expr = expr, from = min(x)-1, to = max(x)+1, add = TRUE, col = "black")
segments(x0 = x, y0 = y, x1 = x, y1 = (y + errors), col = "green",xaxt="n",yaxt="n")

```

## $b_1$ is *Difference* in Conditional Means

```{r x-zero-two,echo=FALSE,fig.align='center',fig.asp=0.9,fig.width=6}
plot(x,y, type = "p",pch = 21, col = "blue", bg = "royalblue",frame.plot = TRUE,cex=1.2,xlim=c(-.2, 1.7), ylim = c(min(y)-.1, max(y)+.1),xlab="is.female",ylab="income",xaxt="n")
axis(side = 1,c(0,1))

points(0, mean(dta[dta$is.female == 0, "income"]), col = 'orange',
       cex = 3, pch = 15)
text(0.35, mean(dta[dta$is.female == 0, "income"]), "E[Y | is.female = 0]", pos = 3)

points(1, mean(dta[dta$is.female == 1, "income"]), col = 'orange',
       cex = 3, pch = 15)
text(0.95, mean(dta[dta$is.female == 1, "income"]), "E[Y | is.female = 1]", pos = 2)
curve(expr = expr, from = min(x)-1, to = max(x)+1, add = TRUE, col = "black")
segments(x0 = x, y0 = y, x1 = x, y1 = (y + errors), col = "green",xaxt="n",yaxt="n")

# red arrow for effect size at xx=1.3
xx = 1.3
arrows(x0 =xx, y0 = mean(dta[dta$is.female == 0, "income"]), x1 = xx, y1 = mean(dta[dta$is.female == 1, "income"]),col="red",lw=3,code=3,length=0.1)
# dashes
segments(x0=0,y0 = mean(dta[dta$is.female == 0, "income"]),x1=xx,y1 = mean(dta[dta$is.female == 0, "income"]),col="red",lty="dashed")
segments(x0=1,y0 = mean(dta[dta$is.female == 1, "income"]),x1=xx,y1 = mean(dta[dta$is.female == 1, "income"]),col="red",lty="dashed")

text(xx, mean(y)+100, paste("b1 =",round(b,2)), pos = 4,col="red")
abline(a=mean(dta$income),b=0,col="blue",lw=2)
```

## Interpretation of Dummy Coefficient $b_1$

>* So, we have seen that 
  $$
  b_1 = E[Y|\text{is.female}=1] - E[Y|\text{is.female}=0]
  $$
>* This was the meaning of the red arrow.

## App!

* Time for you to play around with the Binary Regression!
* Try to find the best line again!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("reg_dummy")
```

# Dummy *and* $X$

## Dummy and other Regressors

> * What if we added $\text{exper}_i\in \mathbb{N}$ to that regression?
    $$
    y_i = b_0 + b_1 \text{is.female}_i + b_2 \text{exper}_i + e_i 
    $$
> * As before, dummy acts as intercept shifter. We have
    $$
    y_i =  \begin{cases}
    b_0 + b_1 + b_2 \text{exper}_i + e_i & \text{if is.female=1} \\
    b_0 + \hphantom{b_1} + b_2 \text{exper}_i + e_i & \text{if is.female=0}
    \end{cases}
    $$
> * intercept is $b_0 + b_1$ for women but $b_0$ for men
> * Slope $b_2$ **identical** for both!

## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("reg_dummy_example")
```

## More than Two Levels: `factor` {.smaller}

>* Sometimes two categories are not enough.
>* The `R` data type `factor` can represent more than just `0` and `1` in terms of categories.
>* Function `factor` takes a numeric vector `x` and a vector of `labels`. Each value of `x` is associated to a `label`:

```{r}
factor(x = c(1,1,2,4,3,4),labels = c("HS","someCol","BA","MSc"))
```

>* `factor` in an `lm` object automatically chooses an omitted/reference category!

# $\log$ Transformations

## $\log$ Removes Scale

>* Let $\alpha$ represent the *scale* of the regression.
>* E.g. if $\alpha=1$, measure in Euros, $\alpha=\frac{1}{1000}$ in thousands of Euros.
>* $\log$ transforming $x$, we obtain
    $$
    y = b_0 + b_1 \log(\alpha x) = b_0 + \log \alpha  + b_1 \log x 
    $$
    hence, scale $\alpha$ moves to intercept, slope becomes *invariant* to it!
    
## $\log$ both Outcome and Regressor

> * Now transform both outcome and regressor:
    $$
    \log y = b_0 + b_1 \log x
    $$
>* Then the slope coefficient is
    $$
    b_1 = \frac{d\log y}{d \log x} = \frac{dy/y}{dx/x}
    $$
>* This represents the **elasticity** of $y$ with respect to $x$
>* $y$ changes by $b_1$ percent if $x$ changes by one percent.

## $\log$ Transformation only on $y$

>* Now *only the outcome* is log transformed.
>* We have a *semi-elasticity* formulation. 
    $$
    \log y = b_0 + b_1 x
    $$
>* Slope coefficient becomes
    $$
    b_1 = \frac{d\log y}{d x}
    $$
>* A one-unit change in $x$ increases the logarithm of the outcome by $b_1$ units.
>* Small $\Delta x$: exponentiating $b_1$ approximates effect of $x$ on *level* of $y$. (not on $\log(y)$)

# Estimating a Wage Equation

## Wage Equation

>* Wages $w$ are often $\log$-transformed
>* $w$ are non-negative
>* $w$ are approximately log-normally distributed (so $\log(w)\sim N$)
>* Here is a typical equation: What's the return of experience?
    $$
    \ln w_i = b_0 + b_1 exp_i + e_i 
    $$
    
## Wage Data

```{r}
data(Wages,package="Ecdat")  # load data
str(Wages)
```
    
## Estimating Wage Equation {.smaller}

```{r}
lm_w = lm(lwage ~ exp, data = Wages)   # setup fit
summary(lm_w)  # show output
```



## Interpreting Results

> * $E[\ln w] = `r round(mean(Wages$lwage),2)`$ and $E[w] = `r round(mean(exp(Wages$lwage)),2)`$
> * Additional year of experience increases $E[\ln w]$ by `r round(coef(lm_w)[2],4)`.
> * Approximate effect on $E[w]=\exp(b_1)=`r round(exp(coef(lm_w)[2]),3)`$
> * Precise effect is $100 * (\exp(b_1)-1) = `r round((exp(coef(lm_w)[2]) -1) * 100,3)`$ 
> * Anway: Tiny effect! `r emo::ji("unhappy")`

## Results in a Picture

```{r wage-plot,fig.align='center',echo=FALSE,message=FALSE,warning=FALSE}
library(ggplot2)
library(dplyr)
ggplot(mapping = aes(y=lwage,x=exp), data=Wages) + geom_point(shape=1,alpha=0.6) + geom_smooth(method="lm",col="red",se=FALSE) + theme_bw()
```

## Different Intercepts by Gender?

> * Let's see if there are important differences by gender in this:
    $$
    \ln w_i = b_0 + b_1 exp_i + b_2 sex_i + e_i 
    $$
>* We just `update` our `lm` object as follows:

```{r}
lm_sex = update(lm_w, . ~ . + sex)  
# update lm_w with same LHS, same RHS, but add sex to it
```

## Interpreting Intercepts by Gender {.columns-2 .smaller} 


```{r,echo=FALSE}
summary(lm_sex)
```

<p class="forceBreak"></p>

>- Notice `sexmale`! `R` appends the offset level to the variable name.
>- Hence, does *not* display omitted category: there is no `sexfemale`.
>- `female` is the **reference category**.
>- Both groups have the **same slope** on `exp`.
>- What does this look like now?

## Picture for Intercepts by Gender

```{r wage-plot2,fig.align='center',echo=FALSE}
p_sex = cbind(Wages,pred=predict(lm_sex))
p_sex = sample_n(p_sex,2500)
p <- ggplot(data=p_sex,mapping=aes(x=exp,y=lwage,color=sex)) 
p + geom_jitter(shape=3,alpha=0.6,width=0.1,stroke=1.1) + geom_line(mapping = aes(y=pred), size=1) + theme_bw()
```
