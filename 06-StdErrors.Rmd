# Standard Errors {#std-errors}

In the previous chapters we have seen how the OLS method can produce estimates about intercept and slope coefficients from data. You have seen this method at work in `R` by using the `lm` function as well. It is now time to introduce the notion that given that $b_0$, $b_1$ and $b_2$ are *estimates* of some unkown *population parameters*, there is some degree of **uncertainty** about their values. An other way to say this is that we want some indication about the *precision* of those estimates. 

```{block,type="note"}
<center>
How *confident* should we be about the estimated values $b$?
</center>
```
<br>
Let's go back to the regression with one variable and remind ourselves of the example at the end of chapter \@ref(linreg). There we introduced the term *confidence interval*, shown here as the shaded area:

```{r,fig.align="center",message=FALSE,warning=FALSE,echo=FALSE}
library(ggplot2)
library(Ecdat)
p <- ggplot(mapping = aes(x = str, y = testscr), data = Caschool) # base plot
p <- p + geom_point() # add points
p <- p + geom_smooth(method = "lm", size=1, color="red") # add regression line
p <- p + scale_y_continuous(name = "Average Test Score") + 
         scale_x_continuous(name = "Student/Teacher Ratio")
p + theme_bw() + ggtitle("Testscores vs Student/Teacher Ratio")
```
  
The shaded area shows us the region within which the **true** red line will lie with 95% probability. The fact that there is an unknown true line (i.e. a *true* slope coefficient $\beta_1$) that we wish to uncover from a sample of data should remind you immediately of our first tutorial. There, we wanted to estimate the true population mean from a sample of data, and we saw that as the sample size $N$ increased, our estimate got better and better - fundamentally this is the same idea here.

## What is *true*? What are Statistical Models?

A **statistical model** is simply a set of assumptions about how some data have been generated. As such, it models the  data-generating process (DGP), as we have it in mind. Once we define a DGP, we could simulate data from it and see how this compares to the data we observe in the real world. Or, we could change the parameters of the DGP so as to understand how the real world data *would* change, could we (or some policy) change the corresponding parameters in reality. Let us now consider one particular statistical model, which in fact we have seen so many times already.

 

## The Classical Regression Model {#class-reg}

Let's bring back our simple model \@ref(eq:abline) to explain this concept.


\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i (\#eq:abline-5)
\end{equation}

The smallest set of assumptions used to define the *classical regression model* as in \@ref(eq:abline-5) are the following:

1. The data are **not linearly dependent**: Each variable provides new information for the outcome, and it cannot be replicated as a linear combination of other variables. We have seen this in section \@ref(multicol). In the particular case of one regressor, as here, we require that $x$ exhibit some variation in the data, i.e. $Var(x)\neq 0$.
1. The mean of the residuals conditional on $x$ should be zero, $E[\varepsilon|x] = 0$. Notice that this also means that $Cov(\varepsilon,x) = 0$, i.e. that the errors and our explanatory variable(s) should be *uncorrelated*. It is said that $x$ should be **strictly exogenous** to the model.

These assumptions are necessary to successfully (and correctly!) run an OLS regression. They are often supplemented with an additional set of assumptions, which help with certain aspects of the exposition, but are not strictly necessary:

3. The data are drawn from a **random sample** of size $n$: observation $(x_i,y_i)$ comes from the exact same distribution, and is independent of observation $(x_j,y_j)$, for all $i\neq j$.
4. The variance of the error term $\varepsilon$ is the same for each value of $x$: $Var(\varepsilon|x) = \sigma^2$. This property is called **homoskedasticity**.


### $b$ is not $\beta$!

Let's talk about the small but important modifications we applied to  model \@ref(eq:abline) to end up at \@ref(eq:abline-5) above:

* $\beta_0$ and $\beta_1$ and intercept and slope parameters
* $\varepsilon$ is the error term.

First, we *assumed* that \@ref(eq:abline-5) is the correct represenation of the DGP. With that assumption in place, the values $\beta_0$ and $\beta_1$ are the *true parameter values* which generated the data. Notice that $\beta_0$ and $\beta_1$ are potentially different from $b_0$ and $b_1$ in \@ref(eq:abline) for a given sample of data - they could in practice be very close to each other, but $b_0$ and $b_1$ are *estimates* of $\beta_0$ and $\beta_1$. And, crucially, those estimates are generated from a sample of data. Now, the fact that our data $(y_i,x_i)$ are a sample from a larger population, means that there will be *sampling variation* in our estimates - exactly like in the case of the sample mean estimating the population average as mentioned above. One particular sample of data will generate one particular set of estimates $b_0$ and $b_1$, whereas another sample of data will generate estimates which will in general be different - by *how much* those estimates differ across samples is the question in this chapter. In general, the more observations we have the greater the precision of our estimates, hence, the closer the estimates from different samples will lie together.


### Standard Errors in Theory {#se-theory}

The standard deviation of the OLS parameters is generally called *standard error*. As such, it is just the square root of the parameter's variance.
Under assumptions 1. through 4. we can define the formula for the variance of our slope coefficient in the context of our single regressor model \@ref(eq:abline-5) as follows: 

\begin{equation}
Var(b_1|x_i) = \frac{\sigma^2}{\sum_i^N (x_i - \bar{x})^2} = \frac{\sigma^2}{Var(x)}  (\#eq:var-ols)
\end{equation}

In pratice, we don't know the theoretical variance of $\varepsilon$, i.e. $\sigma^2$, but we form an estimate about it from our sample of data. A widely used estimate uses the already encountered SSR (sum of squared residuals), and is denoted $s^2$:

$$
s^2 = \frac{SSR}{n-p} = \frac{\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2}{n-p} =  \frac{\sum_{i=1}^n e_i^2}{n-p}
$$
where $n-p$ are the *degrees of freedom* available in this estimation. $p$ is the number of parameters we wish to estimate (here: 1). So, the variance formula would become

\begin{equation}
Var(b_1|x_i) = \frac{SSR}{(n-p)Var(x)}  (\#eq:var-ols2)
\end{equation}

You can clearly see that, as $n$ increases, the denominator increases, and therefore the variance of the estimate will decrease.

### Standard Errors in Practice

We would like to further make this point in an experiential way, i.e. we want you to experience what is going on. We invite you to spend some time with the following apps. In particular, make sure you have a thorough understandign of `launchApp("estimate")`.

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("estimate")
launchApp("sampling")  
launchApp("standard_errors_simple") 
launchApp("standard_errors_changeN") 
```


## Hypothesis Testing

```{r testing,echo=FALSE}
mu = 167
xbar = 168.5
s = 10
n = 50
tstat = round((xbar - mu)/(s/sqrt(n)),3)
ctval = qt(0.95,df=n)
cxbar = ctval * (s/sqrt(n)) + mu
```

Now we know how the standard errors of an OLS estimate are computed, and what they stand for. We can now briefly^[We will not go into great detail here. Please refer back to your first year statistics course or the short note I [wrote while ago](images/hypothesis.pdf) ] discuss a very common usage of this information, in relation to which variables we should include in our regression. There is a statistical proceedure called *hypothesis testing* which helps us to make such decisions.
In [hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), we have a baseline, or *null* hypothesis $H_0$, which we want to confront with a competing *alternative* hypthesis $H_1$. An example concerning the mean height of SciencesPo students ($\mu$) could be

\begin{align}
H_0:& \mu = `r mu`\\
H_1:& \mu \neq `r mu`
\end{align}

This would be called a *two-sided* test, because it tests deviations from $H_0$ below as well as above. An alternative formulation could use the *one-sided* test that

\begin{align}
H_0:& \mu = `r mu`\\
H_1:& \mu > `r mu`.
\end{align}

which would mean: under the null hypothesis, the average of all ScPo students' body height is 167cm. Under the alternative, it is larger. 

At this point you may want to ask: Why bother with this, and not just measure all students' height, compute $\mu$, and that's it? That's a good question! In most cases, we cannot do this, either because we do not have access to the entire population (think of computing the mean height of all Europeans!), or it's too costly to measure everyone, or it's impractical. That's why we take *samples* from the wider population, to make inference. In our example, suppose we'd randomly measure students coming out of the SciencesPo building until we have $`r n`$ measurements. The hypothesis testing methodology is useful to make probabilistic statements about true value of $\mu$ upon observing our sample data. Here, we would compute the sample mean $\bar{x}$ of all height measurements, and ask how informative this is with regards to the above competing hypothesis. Suppose we found that $\bar{x} = `r  xbar`$, and that the sample standard deviation is $s=`r s`$. Would you regard this as strong or weak evidence against $H_0$ and in favor of $H_1$?

You should now remember what you just saw when you did `launchApp("estimate")`. Look again at this app and set the slider to a sample size of $`r n`$, just as in our running example. You can see that the app draws one hundred (100) samples for you, locates their sample mean on the x-axis, and estimates the red density. In reality, you only get to draw one (1!) sample! Let's continue with assuming that our particular samples yields  $\bar{x} = `r xbar`$. 

```{block type='note'}
As a result of the powerful [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), we know the (approximate) sampling distribution of our test-statistic $\bar{x}$! As $n\to\infty$, it converges to the normal distribution with mean $\mu$ (i.e. the **truth**). We say the sample mean is an *unbiased estimator for the population mean*. In the app, the red density approximates this distribution.
```
<br>
Our task then is to decide if given that particular sampling distribution, given our estimate $\bar{x}$ and given an observed sample variance $s^2$, whether  $\bar{x} = `r xbar`$ is *far away* from  $\bar{x} = `r mu`$, or not. The way to proceed is by computing a *test statistic*, which is to be compared to a critical value: if the test statistic exceeds that value, we reject $H_0$, otherwise we cannot.

### Making Errors

There are two types of error one can make when deploying such a test:

1. We might reject $H_0$, when in fact it is true! Here, upon observing  $\bar{x} = `r xbar`$ we might conclude that indeed $\mu > `r mu`$ and thus we'd reject. But we might have gotten unlucky and by chance have obtained an unusually tall sample of students. This is called **type one error**.
2. We might *fail* to reject $H_0$ when in fact $H_1$ is true. This is called the **type two error**.

We design a test with a certain probability of *type one error* $\alpha$ in mind. In other words, we choose with which probability $\alpha$ we are willing to make a type one error. (Notice that the best tests also avoid making type two errors! The number $1-\Pr(\text{type 2 error})$ is called *power*, hence we prefer tests with *high power*). A typical choice for $\alpha$ is 0.05, i.e. we are willing to make a type one error with probability 5%. $\alpha$ is commonly called the *level of significance* of a test.

### Performing the Test


We can stick to the following cookbook procedure, which is illustrated in figure \@ref(fig:testfig). 

1. Set up hypothesis and significance level:
    1. $H_0: \mu = `r mu`$
    2. $H_1: \mu > `r mu`$
    3. $\alpha = 0.05$
2. Test Statistic and test distribution:
    * We don't know the true population variance $\sigma^2$, hence we estimate it via $s^2$ from our sample. 
    * The corresponding test statistic is the *t-statistic*, which follows the [Student's T](https://en.wikipedia.org/wiki/Student's_t-distribution) distribution.
    * That is, our statistic is $T=\frac{\bar{x} - \mu}{s/\sqrt{n}} \sim t_{`r n`}$, where `r n` is equal to our sample size, hence, the *degrees of freedom* in this case.
3. Rejection Region:
    We perform a one-sided test. We said we are happy with a 5% significance level, i.e. we are looking for the $t$ value which corresponds *just* to $1-0.05 = 0.95$ mass under the pdf of the $t$ distribution. More precisely, we are looking for the $1-0.05 = 0.95$ quantile of the $t_{`r n`}$ distribution.^[See the previous footnote for an explanation of this!] This implies a critical value $c = `r round(qt(0.95,df=n),3)`$, which you can verify by typing `qt(0.95,df=50)` in `R`.
4. Calculate our test statistic:
    $\frac{\bar{x} - \mu}{s/sqrt{n}} = \frac{168.5 - 167}{20/\sqrt{50}} = `r tstat`$
5. Decide:
    We find that $`r tstat` < `r round(ctval,3)`$. Hence, we cannot reject $H_0$, because we only found weak evidence against it in our sample of data.


```{r testfig, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', fig.cap='Cookbook Testing Proceedure. Subscripts $c$ indicate *critical value*. There are two x-axis: one for values of $\\bar{x}$, and one for the corresponding $t$ statistic. The red area is the rejection area. If we observe a test statistic such that $t>t_c$, we feel reassured that our $\\bar{x}$ is *sufficiently far away* from the hypothesized value $\\mu$, such that we feel comfortable with rejecting $H_0$. And vice versa: If our test statistic falls below $t_c$, we will not reject $H_0$',fig.align='center',engine.opts = list(convert = 'convert', convert.opts = '-density 600')}
\begin{tikzpicture}[scale=2, y=5cm]
\draw[domain=-3:1.645] plot[id=gauss1, samples=100] (\x,{1/sqrt(2*pi)*exp(-0.5*(\x)^2)});   
\draw[domain=1.645:3.2,fill=red,opacity=0.4] (1.645,0) --
plot[id=gauss3, samples=25] (\x,{1/sqrt(2*pi)*exp(-0.5*(\x)^2)});
\draw (1.645,-0.01) -- (1.645,0.01);    % tick
\node at (1.645,-0.03) {$\bar{x}_c = 171.74$};
\draw[->, semithick] (-3.2,0) -- (3.4,0) node[right]
{$\bar{x}$}; % x-axis 1  
\draw[->, semithick] (-3.2,-0.1) -- (3.4,-0.1) node[right] {$t=\frac{\bar{x}-\mu_0}{\frac{s}{\sqrt{n}}}$}; % x-axis 2

%y-axis
\draw[->, semithick] (-3.15,-0.02) node[left] {$0$} -- (-3.15,0.45)
node[above] {$f(\bar{x}|\mu=167)$}; % y-axis

\draw[-,semithick] (0,-0.01) -- (0,0.01); % zero tick on x
\draw[-,semithick] (0,-0.11) -- (0,0.-0.09); % zero tick on z
\node at (0,-0.03) {$\mu = 167$};
\node at (0,-0.13) {$0$};
\draw[-,semithick] (1.645,-0.11) -- (1.645,-0.09);
\node at (1.645,-0.13)  {$t_c = 1.676$};
% annotate alphas
\node at (0,0.15) {$1 - \alpha = 0.95$};
\draw (2,0.02) -- (2.4,0.1) node[above] {$\alpha = 0.05$};
\end{tikzpicture} 
```

### Testing Regression Coefficients

In Regression Analysis, we often want to test a very specific alternative hypothesis: We want to have a quick way to tell us whether a certain variable $x_k$ is *relevant* in our statistical model or not. In hypothesis testing language, that would be

\begin{align}
H_0:& \beta_k = 0\\
H_1:& \beta_k \neq 0.(\#eq:H0)
\end{align}

Clearly, if in the **true** regression model, $\beta_k=0$, this means that $x_k$ has a zero partial effect on the outcome, hence it should be excluded from the regression. Notice that we are interested in $\beta_k$, not in $b_k$, which is the estimator that we compute from our sample (similarly to $\bar{x}$, which estimates $\mu$ above).

As such, this is a *two-sided test*. We can again illustrate this in figure \@ref(fig:testfig2). Notice how we now have two rejection areas. 

```{r testfig2, echo=FALSE, engine='tikz', out.width='90%', fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png', fig.cap='Testing whether coefficient $b_k$ is *statistically significantly different* from zero. Now we have two red rejection areas. We relabel critical values with a superscript here. If we observe a test statistic falling in either red region, we reject, else we do not. Notice that the true value under $H_0$ is $\\beta_k=0$.',fig.align='center',engine.opts = list(convert = 'convert', convert.opts = '-density 600')}
\begin{tikzpicture}[scale=2, y=5cm]
\draw[domain=-3.15:-1.96,fill=red,opacity=0.4] (-3.15,0) plot[id=gauss1,samples=50]
(\x,{1/sqrt(2*pi)*exp(-0.5*(\x)^2)}) -- (-1.96,0);   
\draw[domain=-1.96:1.96] plot[id=gauss1, samples=100] (\x,{1/sqrt(2*pi)*exp(-0.5*(\x)^2)});   
\draw[domain=1.96:3.2,fill=red,opacity=0.4] (1.96,0) --
plot[id=gauss3, samples=50] (\x,{1/sqrt(2*pi)*exp(-0.5*(\x)^2)});
\draw (1.96,-0.01) -- (1.96,0.01);    % ticks
\draw (-1.96,-0.01) -- (-1.96,0.01);
% ciritcal estimator values
\node at (1.96,-0.03) {$b_k^c$};
\node at (-1.96,-0.03) {$-b_k^c$};

% ticks on t
\draw[-,semithick] (1.96,-0.11) -- (1.96,-0.09);
\node at (1.96,-0.13)  {$t_{up}=1.96$};
\draw[-,semithick] (-1.96,-0.11) -- (-1.96,-0.09);
\node at (-1.96,-0.13)  {$t_{down}=-1.96$};
% x-axis 1
\draw[->, semithick] (-3.2,0) -- (3.4,0) node[right] {$b_k$};  
\draw[->, semithick] (-3.2,-0.1) -- (3.4,-0.1) node[right]
{$t=\frac{b_k-0}{s_k}$}; % x-axis 2
%y-axis
\draw[->, semithick] (-3.15,-0.02) node[left] {$0$} -- (-3.15,0.45)
node[above] {$f(b_k|\beta_k=0)$}; 

\draw[-,semithick] (0,-0.01) -- (0,0.01); % zero tick on x
\draw[-,semithick] (0,-0.11) -- (0,0.-0.09); % zero tick on t
\node at (0,-0.03) {$\beta_k = 0$};
\node at (0,-0.13) {$0$};

% annotate alphas
\node at (0,0.15) {$1 - \alpha = 0.95$};
\draw (2.2,0.02) -- (2.4,0.1) node[above] {$\alpha = 0.025$};
\draw (-2.2,0.02) -- (-2.4,0.1) node[above] {$\alpha = 0.025$};
\end{tikzpicture} 
```


The relevant test statistic for a regression coefficient is again the *t* distribution. In fact, this particular test is so important that all statistical packages report the  *t* statistic corresponding to \@ref(eq:H0) automatically. Let's look at an example:

```{r,echo=FALSE}
lm1=lm(mpg ~ wt +  hp + drat, mtcars)
summary(lm1)
```

The column `t value` is just `Estimate` divided by `Std. Error`. That is, `R` reports in the column `t value` the following number for us:

\begin{equation}
\text{t value} = \frac{b_k}{s_k}  (\#eq:tstat)
\end{equation}

where $s_k$ is the estimated standard error as introduced in \@ref(se-theory). We have to choose a critical value ourselves here. Many people automatically choose the 0.975 quantile of the standard normal distribution, `qnorm(0.975)` `round(r qnorm(0.975),2)` in this case. This is fine for sample sizes greater than 100, say. Here, we only have 28 degrees of freedom, so we better choose the critical value from the *t* distribution as above. We get $t_{down} = `r round(qt(0.025,df=28),3)`$ and $t_{up} = `r round(qt(0.975,df=28),3)`$ as critical values. Let's test 

\begin{align}
H_0:& \beta_{wt} = 0\\
H_1:& \beta_{wt} \neq 0 (\#eq:mtcarswt)
\end{align}

We just take the `t value` entry, and see whether it lies above or below either critical value: Indeed, we see that $-4.053 < `r round(qt(0.025,df=28),3)`$, and we are happy to reject $H_0$.

On the other hand, for `drat` that does not seem to be the case:

\begin{align}
H_0:& \beta_{drat} = 0\\
H_1:& \beta_{drat} \neq 0 (\#eq:mtcarsdrat)
\end{align}

Here we find that $1.316 \in [`r round(qt(0.025,df=28),3)`,`r round(qt(0.975,df=28),3)`]$, hence it does not lie in any rejection region, and we can *not* reject $H_0$.

### P-Values and Stars

`R` also reports two additional columns in its regression output. The so-called *p-value* in column `Pr(>|t|)` and a column with stars. P-values are an improvement over the dichotomy introduced in the standard reject/accept framework above. We never know if we *narrowly* rejected a $H_0$, or not. The p-value is defined as the particular level of significance $\alpha^*$, up to which *all* $H_0$'s would be rejected. If this is a very small number, we have overwhelming support to reject the null. If, on the contrary, $\alpha^*$ turns out to be rather large, we only found weak evidence against $H_0$.

We define the p-value as the sum of rejection areas for a given test statistic $t^*$. Notice that the symmetry of the $t$ distribution implies that we multiply by two each of the two tail probabilities:

\begin{align}
\alpha^* = 2 \Pr(T > |t^*|) 
\end{align}

The stars in the final column are a visualization of this information. They show a quick summary of the magnitude of each p-value. Commonly, `***` mean an extremely small reference significance level $\alpha^*=0$ (almost zero), `**` means $\alpha^*=0.001$, etc. In that case, up to a significance level of 0.1%, all $H_0$ would be rejected. You clearly see that all columns `Std. Error`, `t value` and `Pr(>|t|)` give a different type of the same information.


## What's in my model? (And what is not?)

We want to revisit the underlying assumptions of the classical model outlined in \@ref(class-reg). Right now we to talk a bit more about assumption number 2 of the above definition in \@ref(class-reg). It said this:

```{block type='warning'}
The mean of the residuals conditional on $x$ should be zero, $E[\varepsilon|x] = 0$. This means that $Cov(\varepsilon,x) = 0$, i.e. that the errors and our explanatory variable(s) should be *uncorrelated*. We want $x$ to be **strictly exogenous** to the model.
```
<br>
Great. But what does this *mean*? How could $x$ be correlated with something we don't even observe?! Good questions - let's try with an example.

Imagine that we assume that 

\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i (\#eq:DGP-h)
\end{equation}

represents the DGP of  impact  the sales price of houses ($y$) as a function of number of bathrooms ($x$). We run OLS as

$$
y_i = b_0 + b_1 x_i + e_i 
$$ 
You find a positive impact of bathrooms on houses:

```{r housing,echo=FALSE}
data(Housing, package="Ecdat")
hlm = lm(price ~ bathrms, data = Housing)
summary(hlm)

```

In fact, from this you conclude that each additional bathroom increases the sales price of a house by `r options(scipen=999);round(coef(hlm)[2],1)` dollars. Let's see if our assumption $E[\varepsilon|x] = 0$ is satisfied:

```{r,warning=FALSE,message=FALSE}
library(dplyr)
# add residuals to the data
Housing$resid <- resid(hlm)
Housing %>%
  group_by(bathrms) %>%
  summarise(mean_of_resid=mean(resid))
```

Oh, that doesn't look good. Even though the unconditional mean $E[e] = 0$ is *very* close to zero (type `mean(resid(hlm))`!), this doesn't seem to hold at all by categories of $x$. This indicates that there is something in the error term $e$ which is *correlated* with `bathrms`. Going back to our discussion about *ceteris paribus* in section \@ref(ceteris), we stated that the interpretation of our OLS slope estimate is that 

```{block,type="tip"}
Keeping everything else fixed at the current value, what is the impact of $x$ on $y$? *Everything* also includes things in $\varepsilon$ (and, hence, $e$)!
```
<br>
It looks like our DGP in \@ref(eq:DGP-h) is the *wrong model*. Suppose instead, that in reality sales prices are generated like this:

\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \varepsilon_i (\#eq:DGP-h2)
\end{equation}

This would now mean that by running our regression, informed by the wrong DGP, what we estimate is in fact this:
$$
y_i = b_0 + b_1 x_i + (b_2 z_i + e_i)  = b_0 + b_1 x_i + u_i.
$$ 
This is to say that by *omitting* variable $z$, we relegate it to a new error term, here called $u_i = b_2 z_i + e_i$. Our assumption above states that *all regressors need to be uncorrelated with the error term* - so, if $Corr(x,z)\neq 0$, we have a problem. Let's take this idea to our running example.


### Omitted Variable Bias

What we are discussing here is called *Omitted Variable Bias*. There is a variable which we omitted from our regression, i.e. we forgot to include it. It is often difficult to find out what that variable could be, and you can go a long way by just reasoning about the data-generating process. In other words, do you think it's *reasonable* that price be determined by the number of bathrooms only? Or could there be another variable, omitted from our model, that is important to explain prices, and at the same time correlated with `bathrms`? 

Let's try with `lotsize`, i.e. the size of the area on which the house stands. Intuitively, larger lots should command a higher price; At the same time, however, larger lots imply more space, hence, you can also have more bathrooms! Let's check this out:


```{r,echo=FALSE}
options(scipen=0)
hlm2 = update(hlm, . ~ . + lotsize)
summary(hlm2)
options(scipen=999)
```

Here we see that the estimate for the effect of an additional bathroom *decreased* from `r round(coef(hlm)[2],1)` to `r round(coef(hlm2)[2],1)` by almost 5000 dollars! Well that's the problem then. `r options(scipen=999)`We said above that one more bathroom is worth `r round(coef(hlm)[2],1)` dollars - if **nothing else changes**! But that doesn't seem to hold, because we have seen that as we increase `bathrms` from `1` to `2`, the mean of the resulting residuals changes quite a bit. So there **is something in $\varepsilon$ which does change**, hence, our conclusion that one more bathroom is worth `r round(coef(hlm)[2],1)` dollars is in fact *invalid*! 

The way in which `bathrms` and `lotsize` are correlated is important here, so let's investigate that:


```{r, fig.align='center', fig.cap='Distribution of `lotsize` by `bathrms`',echo=FALSE}
options(scipen=0)
h = subset(Housing,lotsize<13000 & bathrms<4)
h$bathrms = factor(h$bathrms)
ggplot(data=h,aes(x=lotsize,color=bathrms,fill=bathrms)) + geom_density(alpha=0.2,size=1) + theme_bw()
```

This shows that lotsize and the number of bathrooms is indeed positively related. Larger lot of the house, more bathrooms. This leads to a general result:

```{block type='note'}
**Direction of Omitted Variable Bias**

If there is an omitted variable $z$ that is *positively* correlated with our explanatory variable $x$, then our estimate of effect of $x$ on $y$ will be too large (or, *biased upwards*). The correlation between $x$ and $z$ means that we attribute part of the impact of $z$ on $y$ mistakenly to $x$! And, of course, vice versa for *negatively* correlated omitted variables.
```
<br>


