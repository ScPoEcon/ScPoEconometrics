---
title: "ScPoEconometrics6"
author: "Florian Oswald"
date: "`r Sys.Date()`"
output: 
    ioslides_presentation:
        highlight: textmate
        widescreen: true
        logo: ../../images/ScPo.png
        self_contained: false
        css: ../style.css
---

<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Talking about Stars

## What's the meaning of the `***` Stars?  {.columns-2 .smaller}

```{r t1, echo=FALSE}
summary(lm(mpg ~ wt + hp, mtcars))
```
<p class="forceBreak"></p>

>* Up until today, we only looked at column `Estimate`.
>* The remaining three columns refer to the precision of, or our degree of confidence in `Estimate`.
>* Today we'll finally find out why there is *uncertainty* involved in any of this! 
>* `r emo::ji("tada")`


# Statistical Models

## What is a Statistical Model?

<div class="quote-container">
A Statistical Model is a set of assumptions about how the data have been **generated**. It formally describes a *data-generating process* (DGP)
</div>

* Notice the importance of *assumption* in the above definition.
* A Statistical Model is **our** assumption about the DGP. (Assumptions can be more or less appropriate.)

## Classical Model: Minimal Requirements

* The *Classical Regression Model*:
    $$
    y_i = \beta_0 + \beta_1 x_i + \varepsilon_i 
    $$
    Note the $\beta$ instead of the $b$ and the $\varepsilon$ instead of $e$! `r emo::ji("thinking")`

### Minimal Requirements for valid OLS:

* The data are **not linearly dependent**. $\Rightarrow$ *Rank condition*!
* $x$ should be **strictly exogenous** to the model. $E[\varepsilon|x] = 0$
    * This implies $Cov(x,\varepsilon)=0$.

## Additional Assumptions

* The data are drawn from a **random sample** of size $n$.
* The variance of the error term $\varepsilon$ is the same for each value of $x$: $Var(\varepsilon|x) = \sigma^2$. This property is called **homoskedasticity**.
* The errors are *normally distributed*: $\varepsilon \sim \mathcal{N}(0,\sigma^2)$

<div class="centered">
<div class="red">
Minimal Requirements + Additional Assumptions => NORMAL Linear Regression Model.
</div>
</div>

## $b_1$ is not $\beta_1$!

>* $b_1$ *estimates* $\beta_1$. All estimates involve **uncertainty**.
>* Ideally, $b_1$ should be close to $\beta_1$.
>* `r emo::ji("bulb")` For each different sample $\{x_i,y_i\}_{i=1}^N$ we'll get a different $b_1$!
>* In general, the greater $N$, the greater the precision of $b_1$.

## Standard Error of $b_1$

>* Under the previous assumptions, we have
    $$
    Var(b_1|x_i) = \frac{\sigma^2}{\sum_i^N (x_i - \bar{x})^2} 
    $$
>* In practice we don't know $\sigma^2$ and *estimate* it with residuals $e_i$:
    $$
    s^2 = \frac{SSR}{n-p} = \frac{\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2}{n-p} =  \frac{\sum_{i=1}^n e_i^2}{n-p}
    $$
    ($n-p$ in small samples). $s^2$ is the *mean squared error*.

## Standard Error of $b_1$

>* Variance is thus
    $$
    Var(b_1|x_i) = \frac{SSR}{(n-p)\sum_i^N (x_i - \bar{x})^2} 
    $$
>* Standard Error (SE):
    $$
    SE(b_1) = \sqrt{Var(b_1|x_i)}
    $$    
>* You clearly see that the variance shrinks as $n\to\infty$.


## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("estimate")
```

## $b_0$ and $b_1$ are *just like* $\bar{x}$! {.build}

* In the app, you see that larger $N$ implies higher precision of $\bar{x}$
* Density becomes *normal* for large $N$.
* Similar for OLS estimates!

## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_simple") 
```


## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_changeN") 
```

# Detour: Sampling

## Tasked by Monsieur le Directeur

>* Suppose we were asked to estimate the mean height $\mu$ of all SciencesPo students (including regional campuses! `r emo::ji("smile")`
>* Why not just go and measure all students?
>* So we randomly sample 50 students.
>* We find $\bar{x} = 168.5$. So what?

## Test Statistics {.flexbox .vcenter}

<div class="centered">
<div class="red">
You should follow the [book](https://scpoecon.github.io/ScPoEconometrics/std-errors.html#back-to-sampling) from here on! I just put down some keywords on the remaining slides.</div>
</div>

* z-score
* t-statistic

## Confidence Intervals {.flexbox .vcenter}

* We observe variation around $\bar{x} = 168.5$ in our sample
* Can we provide a *range* of confidence where the true $\mu$ will lie with $1-\alpha$ probability?

## Hypothesis Testing {.flexbox .vcenter}

* Competing Hypothesis
* How to confidently choose between them?

## Errors {.flexbox .vcenter}

* Type 1: reject a true H0
* Type 2: fail to reject H0 when H1 is true.

## Performing the test

## Testing Regression coefficient

* This is *very* similar to testing sample means!
* The most important hypothesis for us: is $\beta_k= 0$?

## P-values and Stars

# Omitted Variable Bias

## House prices {.smaller}

```{r,echo=FALSE}
data(Housing, package="Ecdat")
hlm = lm(formula = price ~ bathrms, data = Housing)
summary(hlm)
```

## Checking Conditional Mean Assumption {.smaller}

```{r,echo=TRUE,message=FALSE,warning=FALSE}
library(dplyr)
# add residuals to the data
Housing$resid <- resid(hlm)
Housing %>%
  group_by(bathrms) %>%
  summarise(mean_of_resid=mean(resid))
```

## Wrong Model?

>* What if this is the correct model?
    $$
    y_i = \beta_0 + \beta_1 x_i + \beta_2 z_i + \varepsilon_i 
    $$
>* Well, then what we *identify* is in fact
    $$
    y_i = b_0 + b_1 x_i + (b_2 z_i + e_i)  = b_0 + b_1 x_i + u_i.
    $$ 
>* If we **omit** $z$, it's in the error:
    $$u_i = b_2 z_i + e_i$$
    
## Is omitting $z$ a Problem? {.flexbox .vcenter}

>* Only if $Cov(x,z)\neq 0$.
>* $z$ being part of $e$ then means that $Cov(x,e)\neq 0$ - which we require!
>* So: what could $z$ be in this case?

## Omitting `lotsize` {.flexbox .vcenter}

>* Larger plots of land admit building larger houses.
>* Larger houses are ... larger `r emo::ji("exclamation")` and thus can afford to have more bathrooms!
>* (there is no `surface` in the data, which would be an even better candidate)
>* If indeed `lotsize` is correlated with `bathrooms`, we **cannot make our ceteris paribus claims**!
>* We said: All Else Equal! Well, if you increase `bathrooms`, you tend to also increase `lotsize`!! `r emo::ji("facepunch")`


## Including `lotsize` {.smaller}

```{r,echo=FALSE}
options(scipen=0)
hlm2 = update(hlm, . ~ . + lotsize)
summary(hlm2)
options(scipen=999)
```


## How do $z$ and $x$ covary?

```{r, fig.align='center', warning=FALSE, message=FALSE,echo=FALSE}
library(ggplot2)
options(scipen=0)
h = subset(Housing,lotsize<13000 & bathrms<4)
h$bathrms = factor(h$bathrms)
ggplot(data=h,aes(x=lotsize,color=bathrms,fill=bathrms)) + geom_density(alpha=0.2,size=1) + theme_bw()
```

## Direction of Omitted Variable Bias {.flexbox .vcenter}

>* If $Cov(x,z)>0$, $b_1$ will be *upward biased* (i.e. too high!)
>* If $Cov(x,z)<0$, $b_1$ will be *downward biased* (i.e. too low!)
>* $Cov(x,z)\neq0$ means that we attribute part of $z$'s impact on $y$ to $x$, because they covary!
